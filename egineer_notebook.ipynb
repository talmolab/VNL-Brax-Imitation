{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook For (GPU) Computational Structure of Brain\n",
    "\n",
    "<center><img src=images/rl_icon.png width=30%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dm_control provides a very high level abstraction of Mujoco, which are both implemented by Google Deepmind. **This repository takes a lot of time to parse through it, understand it, and see how each things fit together, but this is very needed as this is the fundamental skills that is needed in research and developing new things:**\n",
    "1. It is the first step towards understanding what you need to do to create.\n",
    "2. It is also generally a good skill to have when learning a very powerful highly abstarcted new tool, parsing through many repository classes, seeing how classes' hierchy is connected and how to find the `right level of abstraction`, seeing inherietence relationships between classes and chasing back all the way to the begining.\n",
    "\n",
    "\n",
    "**Techniqually we can create an species that doesn't exist, using Reinforcement Learning for it to learn (genetic algorithm for evolution?) and then _accept_ that it is how it would act if it actually exist?**\n",
    "\n",
    "Break through sometimes just come at the point where your previous learned knowledge merge on one point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catalog for this Project\n",
    "1. [Stage 1: Building Pipeline Across Systems](#stage-1-building-pipelines-across-systems)\n",
    "    - dm_control Environment Setup\n",
    "    - dm_control Task Level Handling\n",
    "2. [Stage 1: dm_control Arena Backbone Level Engineering](#stage-1-dm_control---arena-backbone-level-engineering)\n",
    "    - Debug Log for \"bridging issue\"\n",
    "3. [Stage 1: Inherent Binding Class for brax Level Envnvironment](#stage-1-inherent-binding-class-for-brax-engineering)\n",
    "    - Debug Log for \"binding issue\"\n",
    "4. [Stage 1: Rendering Check Using Mujoco](#stage-1-rendering-checking-states-in-dm_controlviewer)\n",
    "5. [Stage 2: Walker Class Adapted Engineering](#stage-2-walker-class-adapted-engineering)\n",
    "    - Debug Log for \"updated version\"\n",
    "    - State Object in brax\n",
    "    - Subtree Object in mjx\n",
    "    - Observe Data Feedback\n",
    "    - Rewards Setting\n",
    "    - Mujoco Model Solver Setting\n",
    "6. [Stage 2: Previous Debug Log for \"Rendering Isuue\"](#stage-2-previous-debugging-logs--ideas-for-rendering)\n",
    "7. [Stage 3: ConvNet Adapted](#stage-3-convnet-adaptation)\n",
    "    - Vision Encoder Ideas & Pipelines\n",
    "    - All Data Collected\n",
    "    - Data Flow in brax\n",
    "    - Debug Log for Vision Encoder Input\n",
    "    - Debug Log for Vision Encoder Wrapper Class for Data Pathway Separation\n",
    "    - Debug Log for Vision Encoder Wrapper Class for Network Building\n",
    "    - Debug Log for \"not walking issue\"\n",
    "8. [Stage 3: Vision Network Using `network_base.py`](#stage-3-vision-network-using-network_basepy)\n",
    "    - General Structure\n",
    "    - Modification Try 1\n",
    "    - PPO Model Architecture\n",
    "    - Modification Try 2\n",
    "    - Report from Week 8 (Out->In 神经网络结构)\n",
    "    - Proposition from Week 8 (一体式神经网络结构)\n",
    "    - Small Fixes With Shapes\n",
    "    - Convolution Neural Network Structure\n",
    "    - New Vision Encoder Structure\n",
    "    - Report from Week 9 (内存过大)\n",
    "    - Debugging for GPU Memory Overflow\n",
    "    - Report from Week 10 (内存过大)\n",
    "    - Report from Week 11 (Gap增大，Performance下降)\n",
    "9. [Stage 4: Stac Rodent Model WIth Vision](#stage-4-stac-rodent-model-with-vision)\n",
    "    - Report from Week 1（模型问题）\n",
    "    - Wrapping up GPU Mujoco\n",
    "        - MJX Mujoco & Rendering\n",
    "        - CPU Mujoco\n",
    "        - TorchRL Mujoco\n",
    "        - MJX Mujoco as a poineer\n",
    "10. [Traing Configuration & Rendering Post-trained Rollout](#training-configuration--rendor-trajectory)\n",
    "    - Rendering Rollout in brax\n",
    "11. [Stage 5: Moving to TorchRL Rodent](#stage-5-moving-to-torchrl-rodent)\n",
    "    - TorchRL\n",
    "    - PPO Algorithm Revisited\n",
    "    - Why TorchRL\n",
    "12. [Stage 5: TorchRL Training Loop](#stage-5-torchrl-training-loop)\n",
    "    - Main Training Loop\n",
    "    - PPO_Models\n",
    "    - Proxmial Policy Optimization Loss Functions\n",
    "        - Simplified PPO Algorithm Representation\n",
    "        - Full PPO Algorithm Representation\n",
    "            - Policy Optimization\n",
    "            - Value Gradient Decent\n",
    "    - Environment\n",
    "    - Policy\n",
    "    - Value\n",
    "    - Data Collector\n",
    "    - Replay Buffer\n",
    "    - Loss Function\n",
    "13. [Stage 5: CPU Based Mujoco With TorchRL](#stage-5-cpu-based-mujoco-with-torch-rl)\n",
    "    - Build Modified Mujoco From Source\n",
    "    - ***System is one thing and learning algorithm is the other, both is needed for intelligence to actually occur***\n",
    "14. [Stage 5: Report for TorchRL](#stage-5-report-for-torchrl)\n",
    "    - Report from Week 2 (TorchRL算法)\n",
    "    - Report from Week 3 (CPU训练，环境正确，算法修正)\n",
    "15. [Stage 6: Imitation Learning](#stage-6-imitation-learning)\n",
    "    - Imitation Learning Overview\n",
    "    - Imitation Learning in Brax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collective Efforts\n",
    "This is a lower level abstraction compare to the previous one, but it is still really high level, calling functions and environental setup that dm_control have already implemented.\n",
    "\n",
    "The skill now is to parse through these very convoluted layers to see the `real idea` behind all the implementation and the `pipeline` of how each things are called. In this way, you can know all the way to the implementation level and then decide on which `level of abstraction` you want to stay on to achieve the purpose you need while fixing minimum things and utilize functions that others have already written to complete your goals so\n",
    "\n",
    "> You can spend the time on building on top of it and actually doing more useful things to create new innovations instea of doing redendent work. Then later your work may become a function, a abstraction level, that other people call to build more works in the field. This is a collaboration project that takes a collective effort towards a common goal, this is the powerfullness of python libs, to not reinvent the wheels but use the wheels to build the car, to not recreate data structure for storing data but use the data for more things.\n",
    "\n",
    "Therefore, staying on the right level of abstraction and using implemented functions is very important (just like we used brax implementation of ppo, we can use dm_control's implementation of customized env)\n",
    "\n",
    "- Brax mjx -> Mujoco env -> modify using dm_control implemented packages -> dm_control mjcf\n",
    " - On mjx level? on dm_control implemented env sample level? on mjcf level? on xml level?\n",
    "\n",
    " ***Do notice that these things are already at the task level, which is already an execution of (model + environment), the correct level of `abstraction` should be just the background environment level, not the task level, this is the same with brax***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Stage 1: Building Pipelines Across Systems\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linux GPU Rendoring\n",
    "These are imports, installation, and switch backends needed for `Linux GPU Rendoring`\n",
    "- osmesa is working, all dependencies are installed, but Mujoco rendoring does not support osmesa, only the other ones, which does have installation issues\n",
    "- specifically the `mujoco.viwer` packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `dm_control` Environment\n",
    "References to dm_control implementation of customized environment. These are the super high level libs, locomotion lib should have everything to get a rl agent running in dm control and the mjcf lib should have everything about building a connection class.\n",
    "- [dm_control locomotion lib](https://github.com/google-deepmind/dm_control/tree/main/dm_control/locomotion)\n",
    "- [dm_control mjcf lib](https://github.com/google-deepmind/dm_control/tree/main/dm_control/mjcf)\n",
    "- [dm_control lib](https://github.com/google-deepmind/dm_control/tree/main)\n",
    "- [mujoco](https://mujoco.readthedocs.io/en/3.1.1/overview.html)\n",
    "- [brax](https://github.com/google/brax)\n",
    "\n",
    "\n",
    "- `locomotion class`\n",
    "  - `arena subclass`\n",
    "    - i.e. corridors -> gap corridors\n",
    "  - `walkers subclass`\n",
    "    - assets (xml)\n",
    "    - i.e. rodent -> rat\n",
    "  - `tasks subclass`\n",
    "    - i.e. jump gaps\n",
    "- `mjcf class`\n",
    "  - class we want to build using idea from corridors\n",
    "- `composer class`\n",
    "  - arena.py/arena.xml provides foundation for `locomotion class`\n",
    "\n",
    "  This really took a quite long time to figure out:\n",
    "  1. We can use `tasks_subclass` by converting it to mjcf model, then MjModel\n",
    "  2. Inherentence can be used to build pur own functions needed in `task`, `arena`, and `walker` (work smart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import functools\n",
    "from IPython.display import HTML\n",
    "import PIL.Image\n",
    "import yaml\n",
    "from typing import List, Dict, Text, Callable, NamedTuple, Optional, Union, Any, Sequence, Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapy as media\n",
    "import wandb\n",
    "import os\n",
    "import numpy as np\n",
    "from etils import epath\n",
    "from flax import struct\n",
    "from ml_collections import config_dict\n",
    "import mujoco\n",
    "from mujoco import mjx\n",
    "from dm_control import mjcf as mjcf_dm\n",
    "from dm_control import composer\n",
    "from dm_control.locomotion.examples import basic_rodent_2020\n",
    "from dm_control.composer.variation import distributions\n",
    "from dm_control.locomotion.arenas import corridors as corr_arenas\n",
    "from dm_control.locomotion.tasks import corridors as corr_tasks\n",
    "from dm_control.locomotion.walkers import rodent, ant\n",
    "from dm_control import viewer\n",
    "from dm_control import mujoco as mujoco_dm\n",
    "from dm_control.composer.variation import rotations\n",
    "import jax\n",
    "from jax import numpy as jp\n",
    "from brax import base\n",
    "from brax import envs\n",
    "from brax import math\n",
    "from brax.base import Base, Motion, Transform\n",
    "from brax.envs.base import Env, PipelineEnv, State #MjxEnv, State\n",
    "from brax.mjx.base import State as MjxState\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.io import html, model\n",
    "from brax.io import mjcf as mjcf_brax\n",
    "\n",
    "# !command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
    "# np.set_printoptions(precision=3, suppress=True, linewidth=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `dm_conrol` - Task Level Running\n",
    "This code is directly implementation in the Mujoco framework:\n",
    "\n",
    "There are existing examples of the framework that calls from more basic components of the dm control lib. These examples are already super high level abstractions already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CONTROL_TIMESTEP = .02\n",
    "_PHYSICS_TIMESTEP = 0.001\n",
    "random_state=None\n",
    "\n",
    "walker = rodent.Rat(observable_options={'egocentric_camera': dict(enabled=True)})\n",
    "\n",
    "  # Build a corridor-shaped arena with gaps, where the sizes of the gaps and\n",
    "  # platforms are uniformly randomized.\n",
    "arena = corr_arenas.GapsCorridor(\n",
    "      platform_length=distributions.Uniform(.4, .8),\n",
    "      gap_length=distributions.Uniform(.05, .2),\n",
    "      corridor_width=2,\n",
    "      corridor_length=40,\n",
    "      aesthetic='outdoor_natural')\n",
    "\n",
    "  # Build a task that rewards the agent for running down the corridor at a\n",
    "  # specific velocity.\n",
    "task = corr_tasks.RunThroughCorridor(\n",
    "      walker=walker,\n",
    "      arena=arena,\n",
    "      walker_spawn_position=(5, 0, 0),\n",
    "      walker_spawn_rotation=0,\n",
    "      target_velocity=1.0,\n",
    "      contact_termination=False,\n",
    "      terminate_at_height=-0.3,\n",
    "      physics_timestep=_PHYSICS_TIMESTEP,\n",
    "      control_timestep=_CONTROL_TIMESTEP)\n",
    "\n",
    "env_composed = composer.Environment(time_limit=30,\n",
    "                              task=task,\n",
    "                              random_state=random_state,\n",
    "                              strip_singleton_obs_buffer_dim=True)\n",
    "\n",
    "#viewer.launch(environment_loader=env_composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Stage 1: `dm_control` - Arena Backbone Level Engineering\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "## Debug Log for Initilization of Binding\n",
    "\n",
    "These are just the the backbone skeleton in the [Arena_folder](https://github.com/google-deepmind/dm_control/tree/main/dm_control/locomotion/arenas)\n",
    "- Gap Corridor -> Empty Corridor -> Corridor -> composer.arena -> xml\n",
    "- Rodent -> xml\n",
    "### First Idea:\n",
    "Making the intermediate class that connects the corridor defined environment (corridor environment is in the arena folder, not the task folder)\n",
    "* [Tasks_folder](https://github.com/google-deepmind/dm_control/tree/main/dm_control/locomotion/tasks) is directly running the algorithm already\n",
    "* [Arena_folder](https://github.com/google-deepmind/dm_control/tree/main/dm_control/locomotion/arenas) provides the actual skeleton, this is the correct level of `abstraction` that we are looking for\n",
    "* **Tasks = Walker + Arena**\n",
    "\n",
    "**problem: we can not directly use the arena implemented by dm_control nor can we use the walker implemented by dm_control, they are too specifically designed by dm_control**\n",
    "\n",
    "### Second Idea:\n",
    "1. Directly build an arena through intuitions from the corriodr class\n",
    "2. Link the rodent or humanoid xml to such environment\n",
    "3. because it is created by the mjcf class in dm_control, we can directly export it as a fix file and it cna be utilized in brax\n",
    "    - essentially we skip some of the really convoluted implementation of dm_control from `tasks -> arena + walker -> composer class -> xml`\n",
    "    - we are directly building it from scratch using `mjcf.RootElement()`\n",
    "4. mjcf model here direcly uses Mujoco, there is no need to initiate another mujoco in brax training loop, we directly pass an model into brax\n",
    "\n",
    "**problem: this is way too inefficient and there should be a much smarter way working with it**\n",
    "\n",
    "### Third Idea:\n",
    "1. Seems like that the Corridor class can be directly transfered into a mjcf model by `.to_mjcf_model()`\n",
    "2. Also self implemented a Corridor class that is directly based on `mjcf.RootElement()` and return the model using `.out()` functions\n",
    "3. Try to inherent directly from the Corridor class and assing some more things\n",
    "\n",
    "**problem: so far Inherent class idea is implemented quite well, but binding not assigned**\n",
    "\n",
    "### Forth Idea:\n",
    "1. Directly wrap evrything once `binding` is down with the agent and also the environment at the `tasks` level\n",
    "2. task level can also be exported as an mjcf_model file and the same storing as in MjModel once ptr, same with the brax humanoid that was previously implemented in the training loop.\n",
    "3. Created inherietence from task module and the directly bind it to an existing waler construct file (fix ours later)\n",
    "\n",
    "**problem: this `task wrapper binded model` has friction setup that Mujoco does not support**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Stage 1: Inherent Binding Class for Brax Engineering\n",
    "***\n",
    "\n",
    "### Primary Goal:\n",
    "1. Create direct inheritence that from dm_control (`walker, arena, tasks -> wrapper to physics`)\n",
    "2. Build a pipeline for using customization functions from dm_control for brax training by implementing particular classes that we can change to fit our our needs for the `arena`, `walker`, and `tasks binding` (binds walker and arena)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "## Debug Log for Binding\n",
    "### We need to try to unbound the ant model so it can actually move when trained\n",
    "- attach function documenttaion [here](https://github.com/google-deepmind/dm_control/blob/9e360bf8a069868107e52960a97dcafe2292113f/dm_control/composer/entity.py#L299)\n",
    "- maybe we can use this function [here](https://github.com/google-deepmind/dm_control/blob/9e360bf8a069868107e52960a97dcafe2292113f/dm_control/composer/arena.py#L62), but this function also uses the `attach` function.\n",
    "- cannot initlize the model and then put it back because the output is only the arena file\n",
    "- switch `._mjcf_root` to `.mjcf_model`, the model need to be attached to a valid \"child\" but not fully connected to the ground, `site` should work, it is adding a new object into the arena with certain size: from rendered videos, we can also see that it works.\n",
    "- might be problem with learning to walk in brax then.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gap_Vnl(corr_arenas.GapsCorridor):\n",
    "    def _build(self, corridor_width, corridor_length, visible_side_planes, aesthetic, platform_length, gap_length):\n",
    "        super()._build(corridor_width=corridor_width,\n",
    "                       corridor_length=corridor_length,\n",
    "                       visible_side_planes=visible_side_planes,\n",
    "                       aesthetic = aesthetic,\n",
    "                       platform_length = platform_length,\n",
    "                       gap_length = gap_length)\n",
    "\n",
    "        # self.walker = mjcf.from_path(\"./models/rodent_dm_control.xml\")\n",
    "        # # Initiate walker\n",
    "        # self.spawn_pos = (0, 0, 0)\n",
    "        # self.spawn_site =  self._mjcf_root.worldbody.add('site', pos=self.spawn_pos)\n",
    "        # self.spawn_site.attach(self.walker).add('freejoint')\n",
    "    \n",
    "    def regenerate(self, random_state):\n",
    "        super().regenerate(random_state)\n",
    "\n",
    "# Task now just serve as a wrapper\n",
    "class Task_Vnl(corr_tasks.RunThroughCorridor):\n",
    "    def __init__(self,\n",
    "               walker,\n",
    "               arena,\n",
    "               walker_spawn_position):\n",
    "        \n",
    "        # we don't really need the rest of the reward setup in dm_control, just how the walker is attached to the arena\n",
    "        spawn_site =  arena.mjcf_model.worldbody.add('site', size=[1e-6]*3, pos = walker_spawn_position)\n",
    "        self._arena = arena\n",
    "        self._walker = walker\n",
    "        self._walker.create_root_joints(self._arena.attach(self._walker, attach_site=spawn_site)) # customize starting environment\n",
    "        self._walker_spawn_position = walker_spawn_position\n",
    "        #self._arena.add_free_entity(self._walker)\n",
    "\n",
    "        # Instead of relying on env to instantiate task and bind artificial agent to arena, the task class itself now can bind artificial agent to arena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bowl_Vnl(bowl_arenas.Bowl):\n",
    "    def _build(self, size=(10, 10), aesthetic='default', name='bowl'):\n",
    "        super()._build(size=size, aesthetic=aesthetic, name=name)\n",
    "\n",
    "    def regenerate(self, random_state):\n",
    "        super().regenerate(random_state)\n",
    "\n",
    "class Task_Vnl(bowl_tasks.Escape):\n",
    "    def __init__(self,\n",
    "               walker,\n",
    "               arena,\n",
    "               walker_spawn_position):\n",
    "        \n",
    "        # we don't really need the rest of the reward setup in dm_control, just how the walker is attached to the arena\n",
    "        spawn_site =  arena.mjcf_model.worldbody.add('site', size=[1e-6]*3, pos = walker_spawn_position)\n",
    "        self._arena = arena\n",
    "        self._walker = walker\n",
    "        self._walker.create_root_joints(self._arena.attach(self._walker, attach_site=spawn_site)) # customize starting environment\n",
    "        self._walker_spawn_position = walker_spawn_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilizing Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arena = Gap_Vnl(platform_length=distributions.Uniform(1.5, 2.0),\n",
    "      gap_length=distributions.Uniform(.05, .2),\n",
    "      corridor_width=5, # walker width follows corridor width\n",
    "      corridor_length=40,\n",
    "      aesthetic='outdoor_natural',\n",
    "      visible_side_planes=False)\n",
    "\n",
    "# arena.regenerate(random_state=None)\n",
    "# physics = mjcf_dm.Physics.from_mjcf_model(arena.mjcf_model)\n",
    "# PIL.Image.fromarray(physics.render())\n",
    "\n",
    "walker = ant.Ant(observable_options={'egocentric_camera': dict(enabled=True)})\n",
    "\n",
    "arena_bowl = Bowl_Vnl()\n",
    "\n",
    "task = Task_Vnl(\n",
    "    walker=walker,\n",
    "    arena=arena,\n",
    "    walker_spawn_position=(5, 0, 0))\n",
    "\n",
    "# we need to bind everything to self.arena because that is the only thing we are putting into the MjModel\n",
    "random_state = np.random.RandomState(123456)\n",
    "task.initialize_episode_mjcf(random_state)\n",
    "\n",
    "physics = mjcf_dm.Physics.from_mjcf_model(task.root_entity.mjcf_model) #.root_entity only returns the arena model, no reward, nothing\n",
    "#physics = mjcf_dm.Physics.from_mjcf_model(walker.mjcf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need to step the environment in dm_control then take the model again like this:\n",
    "- walker_joints = walker.mjcf_model.find_all('joint')\n",
    "- physics.bind(walker_joints).qpos = random_state.uniform(size=len(walker_joints))\n",
    "- task.initialize_episode(physics, random_state) # self.walker position changes\n",
    "- physics2 = mjcf_dm.Physics.from_mjcf_model(task.root_entity.mjcf_model)\n",
    "\n",
    "[This](https://github.com/google-deepmind/dm_control/blob/9e360bf8a069868107e52960a97dcafe2292113f/dm_control/composer/entity.py#L299) is how we fixed this issue, we directly change the attach model calling by giving in a predefined existing location in the mjcf file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Stage 1: Rendering checking states in `dm_control.viewer`\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_composed = composer.Environment(time_limit=50,\n",
    "#                               task=task,\n",
    "#                               random_state=random_state,\n",
    "#                               strip_singleton_obs_buffer_dim=True)\n",
    "# viewer.launch(environment_loader=env_composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendoring with Mujoco\n",
    "We want to check if the rendoringproblem after entering brax env is caused by the MjModel itself, so we rendor it here directly fitrst to see if it actually works!\n",
    "\n",
    "- From playing around with the rendering in mujoco, we figured out a key idea: ***camera options in the dm_control MjModel is indicated by numerical number***\n",
    "1. -1 is far side view\n",
    "2. 0 is closer side view\n",
    "3. 1 is back view\n",
    "4. 2 is closer back view\n",
    "5. 3 is egocentric view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mj_model = physics.model.ptr\n",
    "mj_model.opt.cone = mujoco.mjtCone.mjCONE_PYRAMIDAL\n",
    "\n",
    "mjx_model = mjx.put_model(mj_model)\n",
    "mj_data = mujoco.MjData(mj_model)\n",
    "mjx_data = mjx.put_data(mj_model, mj_data)\n",
    "renderer = mujoco.Renderer(mj_model)\n",
    "\n",
    "# enable joint visualization option:\n",
    "scene_option = mujoco.MjvOption()\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_JOINT] = True\n",
    "\n",
    "duration = 5  # (seconds)\n",
    "framerate = 60  # (Hz)\n",
    "\n",
    "frames = []\n",
    "mujoco.mj_resetData(mj_model, mj_data)\n",
    "while mj_data.time < duration:\n",
    "  mujoco.mj_step(mj_model, mj_data)\n",
    "  if len(frames) < mj_data.time * framerate:\n",
    "    renderer.update_scene(mj_data, scene_option=scene_option,camera=0)\n",
    "    pixels = renderer.render()\n",
    "    frames.append(pixels)\n",
    "\n",
    "# Simulate and display video.\n",
    "media.show_video(frames, fps=framerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Stage 2: Walker Class Adapted Engineering\n",
    "***\n",
    "\n",
    "Instead of directly getting the `Mujoco xml` or `mjcf` file, we get it directly by instantiating an customized `dm_control pymjcf` class and then extract it from there directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "## Debug Log: Uopdated Version?\n",
    "- Really funny mistakes that caused 2 hours of debugging: the code took direct inherietence of the previous class, which caused all the problems. However, better understanding of the pipeline and all the complex relationships in Brax. **If there are official changes in Brax, they will notice the user**\n",
    "- All your code should work, even with packages update, the main thing should be fine, just with small update it should work again\n",
    "    - Really ensure that your environment is what you are thinking you are working in\n",
    "    - Keep all the environment setting so if necessary, you can reset it on the site\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Object in Brax\n",
    "[State Object](https://github.com/google/brax/blob/532a88a030a0761f9c83279cd7c5f028bd5aa320/brax/envs/base.py#L35C1-L43C60) contaisn all information about the environment at a given state, it is in the [environment folder](https://github.com/google/brax/blob/532a88a030a0761f9c83279cd7c5f028bd5aa320/brax/envs/base.py). State object inherent the [Base class](https://github.com/google/brax/blob/532a88a030a0761f9c83279cd7c5f028bd5aa320/brax/base.py#L38).\n",
    "- The State object is not refering to the [MjxState Object](https://github.com/google/brax/blob/532a88a030a0761f9c83279cd7c5f028bd5aa320/brax/mjx/base.py#L22), this is just an intermediate step for data passing\n",
    "- The State object is refering to the [Base.State Object](https://github.com/google/brax/blob/532a88a030a0761f9c83279cd7c5f028bd5aa320/brax/base.py#L397)\n",
    "\n",
    "```python\n",
    "class State(base.Base):\n",
    "  \"\"\"Environment state for training and inference.\"\"\"\n",
    "  pipeline_state: Optional[base.State]\n",
    "  obs: jax.Array\n",
    "  reward: jax.Array\n",
    "  done: jax.Array\n",
    "  metrics: Dict[str, jax.Array] = struct.field(default_factory=dict)\n",
    "  info: Dict[str, Any] = struct.field(default_factory=dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtree Object in mjx\n",
    "Let's look at this line:\n",
    "- `com_before = data0.data.subtree_com[1]`\n",
    "\n",
    "### data0\n",
    "- data0 in the implementation is a [`pipeline_state`](https://github.com/google/brax/blob/532a88a030a0761f9c83279cd7c5f028bd5aa320/brax/envs/base.py#L38) object -> which is a [`mjx.base.State`](https://github.com/google/brax/blob/532a88a030a0761f9c83279cd7c5f028bd5aa320/brax/mjx/base.py#L22) object (transition class)->  data of it is `Mjx.data` format\n",
    "\n",
    "\n",
    "### subtree_com[1]\n",
    "- **Center of Mass (COM)**: This is the point in an object or system of objects where the entire mass can be considered to be concentrated. For physical simulations, especially those involving dynamics and kinematics, calculating the COM is essential for accurately determining how forces and movements will affect the object.\n",
    "\n",
    "- **Subtree**: A subtree refers to a part of the overall tree structure. For a physical body, this might mean a limb or a combination of a limb and its subsequent parts. For example, an entire arm, from shoulder to fingertips, can be considered a subtree of the body."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `brax` get_obs Data Feedback\n",
    "This is all the data feed back from the environment for deternmining any rewards:\n",
    "1. data.qpos - **Position**\n",
    "2. data.qvel - **Velocity**\n",
    "3. data.cinert[1:].ravel() - **Inertia Matrix**\n",
    "4. data.cvel[1:].ravel() - **Velocity of Inertia**\n",
    "5. data.qfrc_actuator **Acutator Forces**\n",
    "\n",
    "\n",
    "\n",
    "### Detailed Observation Components:\n",
    "1. **Position**: Extracts the positions (qpos) of the humanoid body. (If self._exclude_current_positions_from_observation is True, it excludes the first two elements of the position vector. This could be useful if you want to exclude certain position information from the observation.)\n",
    "\n",
    "2. **Velocities**: Appends the velocities (qvel) of the humanoid body.\n",
    "\n",
    "3. **Inertia Matrix**: Appends the inertia matrix (cinert) excluding the first row. (This matrix represents the inertia of the body segments.) Inertia helps to examine the distribution of mass in the humanoid and then calculates the relatyionship it would have with the forces that may be generated to accelerate or deaccelerate\n",
    "\n",
    "4. **Velocity of Inertia**: Appends the velocity of the inertia matrix (cvel) excluding the first row.\n",
    "\n",
    "5. **Actuator Forces**: Appends the actuator forces (qfrc_actuator). Actuators are typically modeled as components that generate forces or torques to drive the movement of joints in a simulated robotic system. These forces or torques are applied to the joints of the simulated body, affecting its motion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `brax` Rewards\n",
    "\n",
    "### 3 Main Rewards\n",
    "The reward consists of three parts:\n",
    "- **reward_alive**: Every timestep that the humanoid is alive, it gets a reward of 5.\n",
    "\n",
    "- **forward_reward**: A reward of walking forward which is measured as *1.25 * (average center of mass before action - average center of mass after action) / dt*. *dt* is the time between actions - the default *dt = 0.015*. This reward would be positive if the humanoid walks forward (right) desired. The calculation for the center of mass is defined in the `.py` file for the Humanoid.\n",
    "\n",
    "- **reward_quadctrl**: A negative reward for penalising the humanoid if it has too large of a control force. If there are *nu* actuators/controls, then the control has shape  `nu x 1`. It is measured as *0.1 **x** sum(control<sup>2</sup>)*. ***Essentially control force penalty makes the movement much more realistic***\n",
    "    - Control forces are often the output of a control system or a learned policy (in the case of reinforcement learning), dictating how the entity should move to achieve certain objectives, like walking, jumping, or picking up objects.\n",
    "    - High control forces can lead to aggressive, unstable, or unsafe movements, which might damage the simulated entity or the environment.\n",
    "    - High control forces often result in abrupt, jerky movements that are not only inefficient but can also be unrealistic, especially in simulations aiming to mimic biological movements.\n",
    "\n",
    "### Starting State\n",
    "All observations start in state (0.0, 0.0,  1.4, 1.0, 0.0  ... 0.0) with a uniform noise in the range of [-0.01, 0.01] added to the positional and velocity values (values in the table) for stochasticity. Note that the initial z coordinate is intentionally selected to be high, thereby indicating a standing up humanoid. The initial orientation is designed to make it face forward as well.\n",
    "\n",
    "### Episode Termination\n",
    "The episode terminates when any of the following happens:\n",
    "\n",
    "1. The episode duration reaches a 1000 timesteps\n",
    "2. The z-coordinate of the torso (index 0 in state space OR index 2 in the table) is **not** in the range `[0.8, 2.1]` (the humanoid has fallen or is about to fall beyond recovery)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Mujoco` Model Solver\n",
    "A \"solver\" typically refers to the algorithm or computational method used to resolve the physical equations that govern the dynamics of the system being simulated, including handling contacts, frictions, and constraints among the bodies in the simulation.\n",
    "\n",
    "### Solver Parameters\n",
    "MuJoCo allows users to configure various parameters of the solver to balance between accuracy and performance. These parameters can include:\n",
    "\n",
    "1. **Iterations**: The number of iterations the solver performs to resolve contacts and constraints. More iterations can lead to more accurate results but at the cost of computational time.\n",
    "\n",
    "2. **Tolerance**: The tolerance level for the solver's accuracy. A lower tolerance means the solver will work harder to minimize errors, which can be necessary for precise simulations but again increases computation time.\n",
    "\n",
    "3. **Solver Type**: MuJoCo may offer different types of numerical solvers for the dynamics equations, each with its trade-offs in terms of stability and computational load.\n",
    "\n",
    "### Conjugate Gradient (CG) Solver\n",
    "The Conjugate Gradient solver is one of several solver types available in MuJoCo. It is designed to solve systems of linear equations that arise in the simulation of physical dynamics, particularly when dealing with sparse data structures. The CG solver is often preferred for its efficiency in handling large, sparse systems, which is common in complex simulations involving many joints and contacts.\n",
    "\n",
    "```python\n",
    "mujoco.mjtSolver.mjSOL_CG\n",
    "```\n",
    "\n",
    "### Newton Solver\n",
    "Offers high accuracy at the cost of computational resources, suitable for simulations where precision is paramount.\n",
    "\n",
    "1. High Accuracy: The Newton solver can achieve high levels of accuracy because it iteratively refines its guesses until it converges on a solution that satisfies the system of equations within a predefined tolerance. This characteristic makes it highly effective for simulations where precision is paramount.\n",
    "\n",
    "2. Efficiency in Nonlinear Systems: It is particularly suited for solving nonlinear problems, which are common in physics simulations involving complex interactions between objects. Nonlinear dynamics are typical in systems with large deformations, sophisticated material models, or intricate boundary conditions.\n",
    "\n",
    "3. Convergence Properties: When the initial guess is close to the true solution, the Newton solver converges rapidly due to its quadratic convergence property. However, its performance is highly dependent on the quality of the initial guess and the nature of the system being solved.\n",
    "\n",
    "\n",
    "### PGS (Projected Gauss-Seidel)\n",
    "Good for general-purpose simulations, balancing speed and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Stage 2: Previous Debugging Logs & Ideas for Rendering\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "## Debugging Logs for Rendering:\n",
    "\n",
    "***Key: Debug line by line, get to all the layers, and try one byone to see where the error is actually occuring, then figure out why that is the case, solving a puzzle.***\n",
    "\n",
    "1. Follow error message and function descriptions, check all the way to `mujoco` render level, then to `brax`\n",
    "2. Ran mujoco rnder separately, `MjModel` is able to render\n",
    "3. brax render uses mujoco render, mujoco render works fine, the `MjModel` should be correct, may be problem with `states` in rollout `trajectory`\n",
    "4. problem probably occur on this [line](https://github.com/google/brax/blob/a89322496dcb07ac5a7e002c2e1d287c8c64b7dd/brax/envs/base.py#L214) where `d` has shape of 8 but renderer only takes in shape of 7\n",
    "5. We may need to define our own rendoring function by shrinking some of the inputting array in `d`\n",
    "6. It may be the env setup for brax here is for humanoid, not for ant (humanoid model in dm_control has mj_mopdel.geo_condim issue (mjx only support geo_condium of 3) and rodent has tendon issue)\n",
    "7. Reimplemented ant using a simpler implementation instead of the complex one, seems like still encounter the issue\n",
    "8. May have to reimplement the walker class first for this to work\n",
    "    - check geom_condim\n",
    "    - check tendon\n",
    "9. The mismatch issue may come from mismatch training loop\n",
    "10. previous wrong understanding, training loop for brax env is correct, wrong looking previously because we direcly load in state, not loading in `.Data`\n",
    "11. indeed it is still this [line](https://github.com/google/brax/blob/a89322496dcb07ac5a7e002c2e1d287c8c64b7dd/brax/envs/base.py#L214) that have error occuring, I implemented the rendoring function step by step and it is at this step the error message `ValueError: could not broadcast input array from shape (8,) into shape (7,)` occurs\n",
    "12. Now actually everything works! Practiced debugging going to really small detailed level and trying out bit by bit.\n",
    "\n",
    "### More Understanding\n",
    "What `mjx.get_data` is getting really is the date of one aspect of the rollout, or one state of it `rollout[i]`. so `state.data`\n",
    "\n",
    "```python\n",
    "def render(\n",
    "      self, trajectory: List[base.State], camera: Union[str, None] = None\n",
    "  ) -> Sequence[np.ndarray]:\n",
    "    \"\"\"Renders a trajectory using the MuJoCo renderer.\"\"\"\n",
    "    renderer = mujoco.Renderer(self._model)\n",
    "    camera = camera or -1\n",
    "\n",
    "    def get_image(state: mjx.Data):\n",
    "      d = mjx.get_data(self._model, state) # This line have error occuring, shows same error message\n",
    "      mujoco.mj_forward(self._model, d)\n",
    "      renderer.update_scene(d, camera=camera)\n",
    "      return renderer.render()\n",
    "\n",
    "    return [get_image(s.data) for s in trajectory]  # pytype: disable=attribute-error\n",
    "```\n",
    "\n",
    "#### Idea 1: Know which component is which when calling, take important part that is relevant to debugging and then run separately.\n",
    "\n",
    "#### Idea 2: Take out big chunk of the code and try to remove different things to see what is causing the shape error.\n",
    "- ValueError: could not broadcast input array from is `np.arange(0, nefc * m.nv, m.nv)` into is `result_i.efc_J_rowadr[:]`\n",
    "- **When running the whole structure, the shape seems to match up -> call rendor try -> call mediapy try**\n",
    "\n",
    "#### Idea 3: Separate fundamental functions to examine them separately.\n",
    "Seems like the problem lays in this `mjx.get_data` function, which is [here](https://github.com/google-deepmind/mujoco/blob/8be966cdf9073813ec8b494062f4d97848432057/mjx/mujoco/mjx/_src/io.py#L235) and related to the `get_data_into` function, particularly this [line](https://github.com/google-deepmind/mujoco/blob/8be966cdf9073813ec8b494062f4d97848432057/mjx/mujoco/mjx/_src/io.py#L293). I think this is related to [this line's shape problem](https://github.com/google-deepmind/mujoco/blob/8be966cdf9073813ec8b494062f4d97848432057/mjx/mujoco/mjx/_src/io.py#L284C4-L284C47)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Stage 3: ConvNet Adaptation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `brax` Visual Encoder Ading Idea\n",
    "1. We can use a rendering function in the `Walker` class. but the rendering function itself uses a helper function that sets outside of the `Walker` class.\n",
    "2. We can render from 1st perspective\n",
    "3. We directly intergrade the system as the process decribed below\n",
    "\n",
    "### Implementation Details\n",
    "1. use the same idea with brax `env.render` and grab the mjx.data and pass it through mujoco rendering to get one frame\n",
    "2. real time redner one frame of the state (1st perspective) directly from the one state mjx data at hand and feeds into training loop -> **it is during training, training image, unlike the rollout in the post-training steps**\n",
    "3. add it into the observaryion space in `get_obs`, observation should allow adding\n",
    "4. handle `reset` and `step` function with the new data of the visual cortex\n",
    "5. preprocessing it using a function in the training loop\n",
    "    - pass it through a CNN network or pretrained image encoder (inference) or maybe training it in real time traininga\n",
    "    - we need to find a way to put things into the CNN and **separate the data pathway (key) for visual and for others**\n",
    "        - `make_policy` and put it into the ppo document?\n",
    "        - directly inherent the ppo document and add the vision encoder information we need?\n",
    "        - [dm_control vision encoder link](https://github.com/talmolab/vanilla_rl_rodent/blob/main/main_ppo_rodent_vision.py#L129)\n",
    "        - [brax ppo](https://github.com/google/brax/tree/main/brax/training/agents/ppo)\n",
    "    - We use the same set of training procedure since we want the conv net for maximizing reward as well, so the training for image is exactly the same as for all other data, PPO algorithm, difference being the layer is a AlexNet now.\n",
    "    - Or just extended front connecting to the ppo training network\n",
    "    - vision encoder is \"encoded\" the activation, what the actual PPO takes in is the activation parameter of the CNN, it learns from that, not the raw images, same with the inferenceing function as well.\n",
    "\n",
    "### Pipeline of Visual Encoder\n",
    "\n",
    "<img src=images/encoder_plan_2.png width=60%>\n",
    "<img src=images/encoder_plan_3.png width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All State.Pipeline_state.data colected\n",
    "### All of these are from [mjx.Data](https://github.com/google-deepmind/mujoco/blob/c6a41fbfe64ee7b2680a6bde90200ca660d08c2a/mjx/mujoco/mjx/_src/types.py#L590)\n",
    "\n",
    "```python\n",
    "1. solver_niter=Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=1/0)>\n",
    "2. time=Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\n",
    "3. qpos=Traced<ShapedArray(float32[15])>with<DynamicJaxprTrace(level=1/0)>\n",
    "4. qvel=Traced<ShapedArray(float32[14])>with<DynamicJaxprTrace(level=1/0)>\n",
    "5. act=Traced<ShapedArray(float32[0])>with<DynamicJaxprTrace(level=1/0)>\n",
    "6. qacc_warmstart=Traced<ShapedArray(float32[14])>with<DynamicJaxprTrace(level=1/0)>\n",
    "7. ctrl=Traced<ShapedArray(float32[8])>with<DynamicJaxprTrace(level=1/0)>\n",
    "8. qfrc_applied=Traced<ShapedArray(float32[14])>with<DynamicJaxprTrace(level=1/0)>\n",
    "9. xfrc_applied=Traced<ShapedArray(float32[17,6])>with<DynamicJaxprTrace(level=1/0)>\n",
    "10. eq_active=Traced<ShapedArray(int32[0])>with<DynamicJaxprTrace(level=1/0)>\n",
    "11. qacc=Traced<ShapedArray(float32[14])>with<DynamicJaxprTrace(level=1/0)>\n",
    "12. act_dot=Traced<ShapedArray(float32[0])>with<DynamicJaxprTrace(level=1/0)>\n",
    "13. xpos=Traced<ShapedArray(float32[17,3])>with<DynamicJaxprTrace(level=1/0)>\n",
    "14. xquat=Traced<ShapedArray(float32[17,4])>with<DynamicJaxprTrace(level=1/0)>\n",
    "15. xmat=Traced<ShapedArray(float32[17,3,3])>with<DynamicJaxprTrace(level=1/0)>\n",
    "16. xipos=Traced<ShapedArray(float32[17,3])>with<DynamicJaxprTrace(level=1/0)>\n",
    "17. ximat=Traced<ShapedArray(float32[17,3,3])>with<DynamicJaxprTrace(level=1/0)>\n",
    "18. xanchor=Traced<ShapedArray(float32[9,3])>with<DynamicJaxprTrace(level=1/0)>\n",
    "19. xaxis=Traced<ShapedArray(float32[9,3])>with<DynamicJaxprTrace(level=1/0)>\n",
    "20. geom_xpos=Traced<ShapedArray(float32[38,3])>with<DynamicJaxprTrace(level=1/0)>\n",
    "21. geom_xmat=Traced<ShapedArray(float32[38,3,3])>with<DynamicJaxprTrace(level=1/0)>\n",
    "22. site_xpos=Traced<ShapedArray(float32[35,3])>with<DynamicJaxprTrace(level=1/0)>\n",
    "23. site_xmat=Traced<ShapedArray(float32[35,3,3])>with<DynamicJaxprTrace(level=1/0)>\n",
    "24. subtree_com=Traced<ShapedArray(float32[17,3])>with<DynamicJaxprTrace(level=1/0)>\n",
    "25. cdof=Traced<ShapedArray(float32[14,6])>with<DynamicJaxprTrace(level=1/0)>\n",
    "26. cinert=Traced<ShapedArray(float32[17,10])>with<DynamicJaxprTrace(level=1/0)>\n",
    "27. crb=Traced<ShapedArray(float32[17,10])>with<DynamicJaxprTrace(level=1/0)>\n",
    "28. actuator_length=Traced<ShapedArray(float32[8])>with<DynamicJaxprTrace(level=1/0)>\n",
    "29. actuator_moment=Traced<ShapedArray(float32[8,14])>with<DynamicJaxprTrace(level=1/0)>\n",
    "30. qM=Traced<ShapedArray(float32[81])>with<DynamicJaxprTrace(level=1/0)>\n",
    "31. qLD=Traced<ShapedArray(float32[81])>with<DynamicJaxprTrace(level=1/0)>\n",
    "32. qLDiagInv=Traced<ShapedArray(float32[14])>with<DynamicJaxprTrace(level=1/0)>\n",
    "33. qLDiagSqrtInv=Traced<ShapedArray(float32[14])>with<DynamicJaxprTrace(level=1/0)>\n",
    "34. contact=Contact(dist=Traced<ShapedArray(float32[669])>with<DynamicJaxprTrace(level=1/0)>\n",
    "35. pos=Traced<ShapedArray(float32[669,3])>with<DynamicJaxprTrace(level=1/0)>\n",
    "36. frame=Traced<ShapedArray(float32[669,3,3])>with<DynamicJaxprTrace(level=1/0)>\n",
    "37. includemargin=Traced<ShapedArray(float32[669])>with<DynamicJaxprTrace(level=1/0)>\n",
    "38. friction=Traced<ShapedArray(float32[669,5])>with<DynamicJaxprTrace(level=1/0)>\n",
    "39. solref=Traced<ShapedArray(float32[669,2])>with<DynamicJaxprTrace(level=1/0)>\n",
    "40. solreffriction=Traced<ShapedArray(float32[669,2])>with<DynamicJaxprTrace(level=1/0)>\n",
    "41. solimp=Traced<ShapedArray(float32[669,5])>with<DynamicJaxprTrace(level=1/0)>\n",
    "42. geom1=Traced<ShapedArray(int32[669])>with<DynamicJaxprTrace(level=1/0)>\n",
    "43. geom2=Traced<ShapedArray(int32[669])>with<DynamicJaxprTrace(level=1/0)>)\n",
    "44. efc_J=Traced<ShapedArray(float32[2684,14])>with<DynamicJaxprTrace(level=1/0)>\n",
    "45. efc_frictionloss=Traced<ShapedArray(float32[2684])>with<DynamicJaxprTrace(level=1/0)>\n",
    "46. efc_D=Traced<ShapedArray(float32[2684])>with<DynamicJaxprTrace(level=1/0)>\n",
    "47. actuator_velocity=Traced<ShapedArray(float32[8])>with<DynamicJaxprTrace(level=1/0)>\n",
    "48. cvel=Traced<ShapedArray(float32[17,6])>with<DynamicJaxprTrace(level=1/0)>\n",
    "49. cdof_dot=Traced<ShapedArray(float32[14,6])>with<DynamicJaxprTrace(level=1/0)>\n",
    "50. qfrc_bias=Traced<ShapedArray(float32[14])>with<DynamicJaxprTrace(level=1/0)>\n",
    "51. qfrc_passive=Traced<ShapedArray(float32[14])>with<DynamicJaxprTrace(level=1/0)>\n",
    "52. efc_aref=Traced<ShapedArray(float32[2684])>with<DynamicJaxprTrace(level=1/0)>\n",
    "53. actuator_force=Traced<ShapedArray(float32[8])>with<DynamicJaxprTrace(level=1/0)>\n",
    "54. qfrc_actuator=Traced<ShapedArray(float32[14])>with<DynamicJaxprTrace(level=1/0)>\n",
    "55. qfrc_smooth=Traced<ShapedArray(float32[14])>with<DynamicJaxprTrace(level=1/0)>\n",
    "56. qacc_smooth=Traced<ShapedArray(float32[14])>with<DynamicJaxprTrace(level=1/0)>\n",
    "57. qfrc_constraint=Traced<ShapedArray(float32[14])>with<DynamicJaxprTrace(level=1/0)>\n",
    "58. qfrc_inverse=Traced<ShapedArray(float32[14])>with<DynamicJaxprTrace(level=1/0)>\n",
    "59. efc_force=Traced<ShapedArray(float32[2684])>with<DynamicJaxprTrace(level=1/0)>)\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "In `get_obs` function\n",
    "1. `Position_data` = Traced<ShapedArray(float32[13])>with<DynamicJaxprTrace(level=1/0)>\n",
    "2. `Velocity_data` = Traced<ShapedArray(float32[14])>with<DynamicJaxprTrace(level=1/0)>\n",
    "3. `Image_data` = Traced<ShapedArray(uint8[230400])>with<DynamicJaxprTrace(level=1/0)>\n",
    "\n",
    "In `step` function\n",
    "1. `COM` data is Traced<ShapedArray(float32[17,3])>with<DynamicJaxprTrace(level=1/0)>\n",
    "2. reward is not actually calculated with the `obs` space, **so use dict because not effect?**\n",
    "3. `obs` is a concated data type, **maybe create a `Data` class just like how brax did in there data class?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the direct inherent data class just like mjx.Data\n",
    "class BraxData(mujoco.mjx._src.dataclasses.PyTreeNode):\n",
    "    position:jax.Array\n",
    "    velocity:jax.Array\n",
    "    image:jax.Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Flow in Brax\n",
    "This is the direct inherent data class just like mjx.Data\n",
    "1. Customized data class\n",
    "2. Customized tags\n",
    "3. Easy with calling because of the exact same type\n",
    "\n",
    "Remanber, the official brax tutorial doesn't really use observation data but rather the whole data pool\n",
    "\n",
    "<img src=images/data_flow.png width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How PPO Training Works\n",
    "### Reward flow\n",
    "1. brax ppo `train.py` have important [unroll function](https://github.com/google/brax/blob/3c109cfd131e01691a53891e8f2ec9b32cf97670/brax/training/agents/ppo/train.py#L296) crucial step for generating new action step!\n",
    "2. This leads to the `acting.py` file, which have the [unroll function](https://github.com/google/brax/blob/3c109cfd131e01691a53891e8f2ec9b32cf97670/brax/training/acting.py#L57), which call the [step function](https://github.com/google/brax/blob/3c109cfd131e01691a53891e8f2ec9b32cf97670/brax/training/acting.py#L34). This leads to the [Policy function](https://github.com/google/brax/blob/3c109cfd131e01691a53891e8f2ec9b32cf97670/brax/training/acting.py#L42)\n",
    "3. Reward is gathered in this [Transition Class](https://github.com/google/brax/blob/3c109cfd131e01691a53891e8f2ec9b32cf97670/brax/training/acting.py#L45) for taking gradient later!\n",
    "\n",
    "* One method of approaching separate the vision data and proprioreceptive data is to modify the [data output from the unroll function](https://github.com/google/brax/blob/3c109cfd131e01691a53891e8f2ec9b32cf97670/brax/training/agents/ppo/train.py#L303)\n",
    "\n",
    "### Inputting layer of Neural Network\n",
    "1. [This](https://github.com/google/brax/blob/3c109cfd131e01691a53891e8f2ec9b32cf97670/brax/training/agents/ppo/train.py#L369) is where the NN are made, random initilization to begin with\n",
    "2. Need to also modify the conatiner for all parameters in the [ppo_loss.py](https://github.com/google/brax/blob/3c109cfd131e01691a53891e8f2ec9b32cf97670/brax/training/agents/ppo/losses.py#L31)\n",
    "3. The [optimzer](https://github.com/google/brax/blob/3c109cfd131e01691a53891e8f2ec9b32cf97670/brax/training/agents/ppo/train.py#L237C3-L238C1) takes in the parameters for optimization, which is\n",
    "    ```python\n",
    "    optimizer = optax.adam(learning_rate=learning_rate)\n",
    "    ```\n",
    "    - This should be fine for taking in another set of parameters, reference to [optax](https://github.com/google-deepmind/optax)\n",
    "4. Wrapps again in the [Training State Class](https://github.com/google/brax/blob/3c109cfd131e01691a53891e8f2ec9b32cf97670/brax/training/agents/ppo/train.py#L371) and then [Jax version](https://github.com/google/brax/blob/3c109cfd131e01691a53891e8f2ec9b32cf97670/brax/training/agents/ppo/train.py#L377) it\n",
    "5. Feed into [training_epoch_with_timing](https://github.com/google/brax/blob/3c109cfd131e01691a53891e8f2ec9b32cf97670/brax/training/agents/ppo/train.py#L423)\n",
    "    - Feed in to [training_epoch](https://github.com/google/brax/blob/3c109cfd131e01691a53891e8f2ec9b32cf97670/brax/training/agents/ppo/train.py#L350C14-L350C28)\n",
    "    - [training_epoch](https://github.com/google/brax/blob/3c109cfd131e01691a53891e8f2ec9b32cf97670/brax/training/agents/ppo/train.py#L333) returns ppo_loss matrix\n",
    "\n",
    "6. General Plan:\n",
    "    1. **Separate vision and proprioreceptive State**\n",
    "    2. **Retrieve the parameter once within the training loop using `Training_State.train` function [here](https://github.com/google/brax/blob/3c109cfd131e01691a53891e8f2ec9b32cf97670/brax/training/agents/ppo/train.py#L73)**\n",
    "        - Might need to make certain adjustment to some intermediate data structure\n",
    "        - Might need to re-create `Training_State.train` function\n",
    "        - **Use the same train function but with network adjusted, retrieve parameter, put it into state space, call train function again**\n",
    "    3. **Feed it backin for PPO again**\n",
    "    \n",
    "**Why don't we just do a separte CNN and then encode the activation parameter into state since we are gonna change the algorithm anyways?**\n",
    "\n",
    "**Solution: Modify the [`network.py`](https://github.com/google/brax/blob/3c109cfd131e01691a53891e8f2ec9b32cf97670/brax/training/agents/ppo/networks.py) file directly**\n",
    "- Create a new `network.py` with appropriate modifications for the encoder's stuff.\n",
    "    - Don’t need to go so deep into the `train.py` to change all the training loop\n",
    "    - Brax seems to be very messy but everything is developed in a very modularized and ordered fashion, you can directly guide the input in the `network.py` file and leave the `train.py` file untouched.\n",
    "- The original Brax observation space parameter is a concatenated jax array, but the network should also take in the mjx.data (BraxData) type, or we can just concatenate later if this doesn’t work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "## Debug Log for Vision Encoder Input\n",
    "1. [Brax Reendering](https://github.com/google/brax/blob/3c109cfd131e01691a53891e8f2ec9b32cf97670/brax/io/image.py#L26)\n",
    "2. [Mujoco Rendering](https://github.com/google-deepmind/mujoco/blob/c146bb40033a1e080aa0d881c476e4a0549238be/python/mujoco/renderer.py#L27)\n",
    "3. [Transformation to Egocentric Camera Frame](https://github.com/google-deepmind/dm_control/blob/3adfe8c3a1d1dff491956a290f7205f19dbb55f3/dm_control/locomotion/walkers/base.py#L77)\n",
    "\n",
    "### First Challenge:\n",
    "We need the renderer to take in an `Mujoco.Model` and an `Mujico.Data`, we have the `Mujoco.Model`, but the data in the get_obs function passed in is `pipeline_state. data` format, `pipeline_state` is the state, its data is in `mjx.data` format, we need to convert it to `Mujoco.Data` format.\n",
    "\n",
    "This is resolved as `mjx.data` seems to be also working, but the **problem now goes to jax step**\n",
    "\n",
    "### Second Challenge:\n",
    "There is extra jax array presented with the `brax` `mjx.data` type. In the actual rollout, we append each rollout by stepping the environment, which requires the environment itself.\n",
    "\n",
    "Solved using the new `brax` renderer documentation.\n",
    "```python\n",
    "d = mujoco.MjData(self._model)\n",
    "```\n",
    "\n",
    "image resulted: (240, 320, 3) -> flat out to be an 1D array by making it an jax.numpy.array and then .flatten() it uisng the numpy array function\n",
    "\n",
    "1. **Now that the one frame image can be stored into the observation space, we can start training**\n",
    "2. **Performance time for rendering seems to be quite acceptable, didn't add too much time complexity**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "## Debug Log for Vision Encoder Wrapper Class\n",
    "### For Data Pathway Separation\n",
    "1. We can create a wrapper class to separate the data pathway by creating different `keys` needed and then figure out how to feed into PPO differently.\n",
    "2. Maybe we can directly create such wrapper outside of the env class, a wrapper that saves all aspects of the `Walker.env` class with the only difference being data differences? **key is that this is after the environment is created, add tags?**\n",
    "\n",
    "### Example of Idea2\n",
    "```python\n",
    "class ObsWrapper(...):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'velocity': ...,\n",
    "            'position': ...,\n",
    "            'image': ...,\n",
    "        })\n",
    "```\n",
    "\n",
    "- We make **an dictionary** to tag and separate data now (**directly in the get_obs function**, canm make it a data class if needed for later usage)! Now data are separated\n",
    "```python\n",
    " observation = {'vision': image_jax,\n",
    "                   'proprioceptive': jp.concatenate([\n",
    "                     position,\n",
    "                     velocity,\n",
    "                     ])}\n",
    "    return observation\n",
    "```\n",
    "- However, obs can only take in an long lenagth of array, maybe chunk it later?\n",
    "- We directly package our data just like mjx.Data but using a different inheritence class, this way we cna customize **data class**, customized **tags**, and directly have **separated data**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "## Debug Log for Vision Encoder Building Network\n",
    "\n",
    "[This](https://github.com/google/brax/blob/3c109cfd131e01691a53891e8f2ec9b32cf97670/brax/training/agents/ppo/networks.py#L37) is how the PPO makes a policy netweork, having `type.Observation` input\n",
    "\n",
    "```python\n",
    "def make_ppo_networks(\n",
    "    observation_size: int,\n",
    "    action_size: int,\n",
    "    preprocess_observations_fn: types.PreprocessObservationFn = types\n",
    "    .identity_observation_preprocessor,\n",
    "    policy_hidden_layer_sizes: Sequence[int] = (32,) * 4,\n",
    "    value_hidden_layer_sizes: Sequence[int] = (256,) * 5,\n",
    "    activation: networks.ActivationFn = linen.swish) -> PPONetworks:\n",
    "  \"\"\"Make PPO networks with preprocessor.\"\"\"\n",
    "  \n",
    "  parametric_action_distribution = distribution.NormalTanhDistribution(event_size=action_size)\n",
    "  \n",
    "  policy_network = networks.make_policy_network(\n",
    "      parametric_action_distribution.param_size,\n",
    "      observation_size,\n",
    "      preprocess_observations_fn=preprocess_observations_fn,\n",
    "      hidden_layer_sizes=policy_hidden_layer_sizes,\n",
    "      activation=activation)\n",
    "  \n",
    "  value_network = networks.make_value_network(\n",
    "      observation_size,\n",
    "      preprocess_observations_fn=preprocess_observations_fn,\n",
    "      hidden_layer_sizes=value_hidden_layer_sizes,\n",
    "      activation=activation)\n",
    "\n",
    "      return PPONetworks(\n",
    "      policy_network=policy_network,\n",
    "      value_network=value_network,\n",
    "      parametric_action_distribution=parametric_action_distribution)\n",
    "```\n",
    "\n",
    "We proabably need to write our own unique function of `make_policy_network`\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "## Debug Log for Not Walking\n",
    "1. Not a binding issue, rendering with random control force show that it is moving + drop from height works, not attached to the arena. **Not a env defining issue**.\n",
    "2. Not a observation space mismatch issue because observations pace tells the agent all information about itself in the environment, if healthy reward is working, observation space is matching because it have propriorteeptive input. **Not a Brax env issue**\n",
    "3. Does the agent have sufficient information about it's velocity and walking forward? Does it know that? Brax treat the whole \"Walker Class\" as an agent and define reward in that sense, `self` is refering to the artificial agent. However, we passed in the arena with the walker into it.\n",
    "    - The ant walks just as fine when it is passing in just the ant model and also when passing in as the arena binding model. It may not be the issue of model passed into the brax class because randomly initiate control forces does pass in to the walker correctly.\n",
    "    - The velocity (key for deciding forward reward) does look at the center of mass movement for the torso, we need to identiy which center of mass we are looking at. the center of mass has an array of float32[17,3], we need to find **the right center of mass**.\n",
    "\n",
    "### Findings:\n",
    "1. Reward assigning need to much more specific, we are dealing with a much more complicated situation where the passed in model is a arena and a walker combined.\n",
    "    - They are not fully bind, we need to extract each's correct sub components to assign reward, much more accurate reward assigning (COM check maybe?)\n",
    "    - Some reward working, observation space feedback is successful\n",
    "    - In Brax, the class is designed directly for one artificial agent, so grabbing all teh COM and assigning reward to `self` is much more simple, but in thi  modified case, we are using the class as a monitor of the full environment, so reward assigning must be precise to the correct sub_tree section.\n",
    "2. The brax class now is a big monitor of both the `arena` + `the walker`, need to make sure reward assign to correc `sub_tree`.\n",
    "3. This is the correct perspective, changing COM does now give non-zero distance and forward reward! Maybe we can use `subtree_linvel`\n",
    "    - The subtree linear velocity represents the velocity of all the connected bodies under the specified body.\n",
    "    - Actually no, this requires using the physics module directly, which we don't have\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Stage 3: Vision Network using `network_base.py`\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Structure\n",
    "1. Big class is the `class PPONetworks` which contains 3 network\n",
    "2. `make_ppo_networks` instantiate all the networks using the base `networks.py` file and return the class wrapper object\n",
    "3. `make_inference_fn` is the main training function\n",
    "    - Brax data return to PPO familiar form in here\n",
    "    - vision network instantiated first to retrieve parameter\n",
    "    - concatinated with velocity and position data to feed into `train.py` in ppo calling\n",
    "\n",
    "* `train.py`'s function of train is called with extra parameter feeded in with the customized file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do we need to step into the environment for getting more \"stepped\" data for vision feedin? Or is parameters good enough?**\n",
    "\n",
    "```python\n",
    " # ppo.train file calls makes policy, which gets the post processed actions and a dictionary, we do the same here\n",
    "       if deterministic:\n",
    "         return ppo_networks.parametric_action_distribution.mode(visio_param), {}\n",
    "       raw_actions = parametric_action_distribution.sample_no_postprocessing(\n",
    "         visio_param, key_sample)\n",
    "       log_prob = parametric_action_distribution.log_prob(visio_param, raw_actions)\n",
    "       \n",
    "       postprocessed_actions = parametric_action_distribution.postprocess(raw_actions)\n",
    "       dict = {'log_prob': log_prob, 'raw_action': raw_actions}\n",
    "\n",
    "       next_state, data = acting.generate_unroll(\n",
    "          env,\n",
    "          current_state,\n",
    "          policy,\n",
    "          current_key,\n",
    "          unroll_length,\n",
    "          extra_fields=('truncation',))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "## Modification Try 1\n",
    "1. Currently made fake image (rand pixel) for testing the pipeline\n",
    "2. trasnitions seems to be okay but there is a shape error where `ValueError: Incompatible shapes for broadcasting: shapes=[(128, 230400), (230427,)]`\n",
    "    - maybe caused by `env_state.obs.shape[-1]` in train.py where the observation space defined in training is either **inconsistent (image + proprioreceptive)** or the **data format in shape attribute** that is returned by customized BraxData is wrong.\n",
    "3. DataClass.image is different in `base.py` and in `network_vision.py`\n",
    "    - Traced<ShapedArray(float32[230400])>with<DynamicJaxprTrace(level=3/0)>\n",
    "    - Traced<ShapedArray(float32[128,230400])>with<DynamicJaxprTrace(level=3/0)>\n",
    "    - This is due to the `vmap` function of cutting to smaller pieces\n",
    "    - `vmap` treat concatenated jax array or brax data class the same, all split\n",
    "    - shape mismatch issue still exist!\n",
    "    - Resolved this issue!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Model Architecture\n",
    "This is the model architecture that is used:\n",
    "\n",
    "```python\n",
    "RunningStatisticsState(mean=Traced<ShapedArray(float32[230427])>with<DynamicJaxprTrace(level=1/0)>, std=Traced<ShapedArray(float32[230427])>with<DynamicJaxprTrace(level=1/0)>, count=Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=1/0)>, summed_variance=Traced<ShapedArray(float32[230427])>with<DynamicJaxprTrace(level=1/0)>) {'params': {'hidden_0': {'bias': Traced<ShapedArray(float32[256])>with<DynamicJaxprTrace(level=1/0)>, 'kernel': Traced<ShapedArray(float32[230427,256])>with<DynamicJaxprTrace(level=1/0)>}, 'hidden_1': {'bias': Traced<ShapedArray(float32[256])>with<DynamicJaxprTrace(level=1/0)>, 'kernel': Traced<ShapedArray(float32[256,256])>with<DynamicJaxprTrace(level=1/0)>}, 'hidden_2': {'bias': Traced<ShapedArray(float32[256])>with<DynamicJaxprTrace(level=1/0)>, 'kernel': Traced<ShapedArray(float32[256,256])>with<DynamicJaxprTrace(level=1/0)>}, 'hidden_3': {'bias': Traced<ShapedArray(float32[256])>with<DynamicJaxprTrace(level=1/0)>, 'kernel': Traced<ShapedArray(float32[256,256])>with<DynamicJaxprTrace(level=1/0)>}, 'hidden_4': {'bias': Traced<ShapedArray(float32[256])>with<DynamicJaxprTrace(level=1/0)>, 'kernel': Traced<ShapedArray(float32[256,256])>with<DynamicJaxprTrace(level=1/0)>}, 'hidden_5': {'bias': Traced<ShapedArray(float32[16])>with<DynamicJaxprTrace(level=1/0)>, 'kernel': Traced<ShapedArray(float32[256,16])>with<DynamicJaxprTrace(level=1/0)>}}}\n",
    "```\n",
    "This is where the `230427` comes from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "## Modification Try 2\n",
    "1. The current implementation `_unpmap` the jax array and then `_re_vmap` it later on after everything is being processed\n",
    "2. All pipeline working, but encountering the issue where in some of the code that the basic brax training is using `shape` of the obs, which is intentionally to be a **concatenated jax array** that is changing due to vmap. However, this changing mechanism changes when we just implement a shape attribute in the **BraxData class**.\n",
    "    - `TypeError: cannot reshape array of shape (128,) (size 128) into shape [230427] (size 230427)`\n",
    "    - This is just getting index 0 instead of index 1, it is flipped\n",
    "    - actually this may be caused by the shape tuple not getting the `vmap`, it need to get `vmap`\n",
    "    - shape[-1] might be used to avoid `vmap` issue to always get the observation space size\n",
    "    - There is a type check `TypeError: where requires ndarray or scalar arguments, got <class 'vnl_brax.data.BraxData'> at position 1.`\n",
    "    - This occurs becaus  the function is using `jp.where`, which only works on array\n",
    "3. Reduced problem of doing not mapping and then mapping by using `axis=1`\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report from Week 8 (Out->In 神经网络结构)\n",
    "Modularized Training:\n",
    "\n",
    "1. Successful in building up pipeline and dealing with `vmap` situation and matching dimensions -> everything in `network.py` file is good other than need to customize parameter size a bit more.\n",
    "2. Need to find a way to modify the `network.py` file better\n",
    "    - customized parameter size\n",
    "    - impute the (27 proprioception + 16 vision activation) array right back to the observation space\n",
    "3. Notice there are 2 environment, one `env_state` and one `env_eval`, `env_eval` can be passed in separately\n",
    "    - There is a `type` check due to the use of `jp.where()` later on in `eval_env`\n",
    "    - Resolving by using the Brax data for the purpose of policy and then switch bakc to the brax_original_form data after making the policy?\n",
    "    - `eval_env` is not instantiated, it is a pure `Walker` class\n",
    "    - This can be resolved by modifying the `unroll` function in the `acting.py` file, but this would encounter issues when passing back to the `network.py` file again.\n",
    "    - changed the after processed state obs space back to th  original obs space (need to modify later)\n",
    "    - **problem**:\n",
    "    \n",
    "```python\n",
    " # this function calls to network.py file -> need BraxData while need d-array, problematic\n",
    "(final_state, _), data = jax.lax.scan(\n",
    "    f, (env_state, key), (), length=unroll_length)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposition from Week 8 (一体式神经网络)\n",
    "1. This is not a problem with `BraxData` class as we need to impute back such result anyways, the key is making sure that we change both of the environment once the activation parameter is retrieved, we also need to consider whether just these activation is enough and whether th egradient can update them and feed back to vision network\n",
    "\n",
    "2. We may need to consider having a separate **conv_net** in a separate file that trains the cnn then feeding it into the `obs_space`\n",
    "3. We need to re-consider the whole  architecture of the network, **can we add conv layer diretly and leaving the rest as it is, then just specify the input? not out put of one as the input of another, but rather one network as a whole** -> (check `.apply` functions in the basic network file. See how to manipulate obs apply to which layer.) -> [一体式神经网络](https://github.com/google/flax/blob/main/examples/ppo/models.py)\n",
    "    - -> the current method is doing way to much modifications\n",
    "    - -> may also need to adjust the BraxData class to just makeit easier, just use an array and cut it later\n",
    "    - -> minimize touching of brax's file\n",
    "\n",
    "**The basic `network.py` file in brax uses a jax supported `linen.Module`, which create a class and allow the input to be a **jax array**, we can customize where the data goes by designing the data flow in the class.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### Really great experience on understanding all the flow of data, the complexity of the project, **right level of abstarction**, **involve in ML pipeline experience**, and **seeing the collective effort**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start to look at this method when thinking about how does the `param` gets distrbuted with the `.apply` function**\n",
    "\n",
    "After building the network and then solving some intergarting issues with:\n",
    "- feed_in data dimension,\n",
    "- 2 dimension slicing,\n",
    "- parameter feedin tuning,\n",
    "- reshaping image data to `vmap` form acceptable as well\n",
    "- `vmap` data handling (causing unequal split problem)\n",
    "\n",
    "The pipeline seems to work! One iteration performed!\n",
    "\n",
    "**after working with it for a while, you know kind of what maight be causing the actual bug (i.e. some array only have one dim))**\n",
    "\n",
    "\n",
    "<img src=images/vision_encoder.png width=90%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Fixes with Shape\n",
    "- `network_vision.py` file is not called by the ppo `train.py` directly in import, but it is feed in through the `network_factory` label!\n",
    "```python\n",
    "TypeError: cannot reshape array of shape (5, 485, 230427) (size 558785475) into shape (5, 240, 320, 3) (size 1152000)\n",
    "```\n",
    "- This is a dynamic increasing aray, this is why [-1] is used to access obs space size previously in the `train.py`.\n",
    "```python\n",
    "TypeError: cannot reshape array of shape (5, 485, 230427) (size 558785475) into shape (-1, 240, 320, 3) because the product of specified axis sizes (230400) does not evenly divide 558785475\n",
    "```\n",
    "- figure out a good share or maybe no need to reshape at all since `vmap` is already applied!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Structure\n",
    "* Convolutional Layers: Three convolutional layers are applied in sequence to vision_data, each followed by a ReLU activation function. These layers progressively apply filters to extract features from the input images:\n",
    "\n",
    "    - The first conv layer applies 32 filters of size (8, 8) with a stride of (4, 4).\n",
    "    - The second conv layer applies 64 filters of size (4, 4) with a stride of (2, 2).\n",
    "    - The third conv layer applies 64 filters of size (3, 3) with a stride of (1, 1).\n",
    "\n",
    "* Flattening: After passing through the convolutional layers, vision_data is flattened into a two-dimensional array with the second dimension being 76800. This operation is crucial for transitioning from convolutional layers to fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Vision Encoder Architecture\n",
    "<img src=images/vision_encoder_updated.png width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report from Week 9 (内存过大)\n",
    "1. Pipeline in general works! handling spltting batches in image data also works!\n",
    "2. memory issue encountered\n",
    "3. need to solve rendering issue\n",
    "4. Consider to reduce parralel env\n",
    "5. Consider chaneg mjx.render function with new updated version\n",
    "\n",
    "<img src=images/gpu_overload.png width=90%>\n",
    "\n",
    "- Some rendering tips in [here](https://pytorch.org/rl/reference/generated/knowledge_base/MUJOCO_INSTALLATION.html) from Charles\n",
    "- Switch `jax.lax.scan` or other methods for not for looping\n",
    "- Switch to mujoco renderer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "## Debugging for Memory Overload\n",
    "- Incremental Development:\n",
    "    - ConvNet shape doesn't seem to be too big\n",
    "    - Checking without ConvNet just having Proprioreceptive\n",
    "\n",
    "\n",
    "**There are no `for loop issue` and `reshape issue`. This is from running just the proprioreceptive data over the network, no ConvNet, seems like the problem is with the `original obs_array size`, we may want to reduce the image size by a bit**\n",
    "- Consider 64 x 64 x 3 array?\n",
    "- `this is out of full ppo network (5, 64, 256)` -> `this is out of full ppo network (5, 1, 1)`\n",
    "\n",
    "```python\n",
    "Peak buffers:\n",
    "        Buffer 1:\n",
    "                Size: 8.79GiB\n",
    "                Operator: op_name=\"pmap(training_epoch)/jit(main)/while/body/transpose[permutation=(0, 2, 1, 3)]\" source_file=\"/home/jovyan/Brax-Rodent-Run/train.py\" source_line=110\n",
    "                XLA Label: fusion\n",
    "                Shape: f32[32,64,5,230427]\n",
    "                ==========================\n",
    "\n",
    "        Buffer 2:\n",
    "                Size: 8.79GiB\n",
    "                Operator: op_name=\"pmap(training_epoch)/jit(main)/while/body/transpose[permutation=(0, 2, 1, 3)]\" source_file=\"/home/jovyan/Brax-Rodent-Run/train.py\" source_line=110\n",
    "                XLA Label: fusion\n",
    "                Shape: f32[32,64,5,230427]\n",
    "                ==========================\n",
    "\n",
    "        Buffer 3:\n",
    "                Size: 8.79GiB\n",
    "                Operator: op_name=\"pmap(training_epoch)/jit(main)/while/body/while/body/jit(_take)/select_n\" source_file=\"/home/jovyan/Brax-Rodent-Run/train.py\" source_line=110\n",
    "                XLA Label: fusion\n",
    "                Shape: f32[1,2048,5,230427]\n",
    "                ==========================\n",
    "```\n",
    "\n",
    "- This idea is very correct! -> when reducing image size, training works!\n",
    "- mjx rendering with rendering tips in [here](https://pytorch.org/rl/reference/generated/knowledge_base/MUJOCO_INSTALLATION.html) from Charles works now! -> image slice to 64x64x3\n",
    "- There are quite some big update on Brax's MJX backend training loop after the update, but the vision network seems to work pretty well!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report from Week 10 (内存过大)\n",
    "1. The above idea is very correct! -> when reducing image size, training works!\n",
    "2. mjx rendering with rendering tips in [here](https://pytorch.org/rl/reference/generated/knowledge_base/MUJOCO_INSTALLATION.html) from Charles works now! -> image slice to 64x64x3\n",
    "3. There are quite some big update on Brax's MJX backend training loop after the update, but the vision network seems to work pretty well!\n",
    "4. Remanber all jax's reward is not calculated from the obs space but rather from the data, which is collecting all environment information\n",
    "    - [jax.lax.scan](https://github.com/google/brax/blob/b68d9387f8c0b05271e0a5fd4cff8f851a256995/brax/envs/base.py#L121) in the `pipeline_step` function is where the error comes from\n",
    "    - Previously have not dealing with the data side directly, no `step` function is called directly\n",
    "    - This is not a problem with the image, the same error occurs even when just proprioreceptive data is feed into it!\n",
    "5. Seems like the leak tracer may be coming from not `jit` correctly according to issues on brax\n",
    "6. Import on brax seems to behave a bit not correct!\n",
    "7. Both problem resolved, it is a problem with brax's dependency with flax -> `flax`, `jax` need to be updated more often when `brax` is updated\n",
    "8. Add distance reward using distance from origin, not velocity -> count negative **do the eucledian distance from the origin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report from Week 11 (Gap增大，Performance下降)\n",
    "1. Starting to make an bowl escape class from dm_control for multiple class training\n",
    "2. Seems like `mjGEOM_HFIELD` and `mjGEOM_SPHERE` is not supported for collision in mjx, but it is in mjx (**Heightfield collision is not supported in mjx yet**)\n",
    "3. Now consider having two training in sequence with different gap length? then do neural representation analysis?\n",
    "    - COM changes\n",
    "    - Solver changes\n",
    "4. Considering to increase gap length for training to try to see if there would be gap behavior\n",
    "5. Added eucledian distance\n",
    "6. Figuring out COM is really hard (should link to Brax, not dm_control)\n",
    "7. The gap ant is not learning when 1. gap too big? 2. euclidean distance getting too much punishment\n",
    "    - Using exact implementation hyperparameter that dm_control uses now.\n",
    "8. **Seems like the CNN is not working too well**:\n",
    "    - maybe trying edge detector?\n",
    "    - maybe trying pretrained model?\n",
    "    - maybe trying gey-scale of the full image?\n",
    "    - maybe feature extraction isn't working? perhaps need to consider how to assess if image really working?\n",
    "    - ask advices from Charles\n",
    "    - **Ant may not actually be able to jump gaps!** -> Best is to cross gap!\n",
    "    - **As the gap increases and vision implemented, the ant doesn't cross the gap anymore, instea just:**\n",
    "        - Not moving at all -> doesn't know how to walk\n",
    "        - Move in the opposite direction\n",
    "9. Adapting to rodent model now! Rodent model from XML itself works, random ctrl works! Try to adapt to dm_control\n",
    "    - rendering about 5m\n",
    "    - Brax deosn't have tendon implemented, need to disable it in the dm_control `rodent.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Stage 4: Stac Rodent Model With Vision\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report From Week 1（模型问题）\n",
    "1. Need to intergrate the stac mjx rodent model with dm_control\n",
    "    - When converting to `physics` model, the root becomes `walker/torso` ect because is it is a task model now binded with the arena, so specification is automatically applied.\n",
    "    - Using package directly work just fine, but doesn't work when using customized local version\n",
    "    - All seems to be the same, but still doesn't work, ask charles\n",
    "    - It is a problem with free joint definition, charles solved this issue\n",
    "2. Rendering issue with shape mismatch\n",
    "    - Shape correct in obs space\n",
    "    - Shape correct in starting of ConvNet\n",
    "    - Issue occur when cutting proprioreceptive, rodent have more proprioreceptive data than ant, ant have 27, rodent have more. Solve issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up GPU Mujoco Tasks\n",
    "### MJX Side of Improvement:\n",
    "1. Solver issue: CG solver is consistent and fast enough, Newton solver technically is better on GPU, but this doesn’t work for the rodent model, doesn’t converge -> lead to Nan condition at lower iterations (maybe because of model complexity?)\n",
    "2. Reset environment issue?\n",
    "3. Rendering?\n",
    "\n",
    "### MJX Rendering Issue:\n",
    "1. We are using Mujoco render, no jax implementation of rendering, not jit compilable or vectorizable\n",
    "Mj_data not updated, kee using initial one\n",
    "2. Need to get MJX data updated\n",
    "    - In  a jitted environment, data is not being computed, it is just traced (Mujoco looking for actual data, but MJX is like being on highway, it is imagining all the computation, but not doing it, it is just giving traced imaginary data, Mujoco would give error because MJX data is technically not really giving anything)\n",
    "    - Charles have a implementation getting around this error, getting data out of the jitted thing, Brax have rendering as well\n",
    "    - The problem is that it is inefficient, jax is good for parallel using V-Map, using call back runs the code in sequential, which is slow\n",
    "    - rendering  is not currently supported in jax-lizable format, single rollout is okay, but per environment step (control step, 5 env step) not good, 2 millisecond per env step.\n",
    "    - Maybe every few steps rendering, one image per 50 millisecond. -> maybe try it and see at what interval is good? In theory every 50 control step may work (recover speed) -> every 30 seconds an image\n",
    "        - Currently implementing a new self.feature to do it.\n",
    "3. Made a new python file `base.py` for new rendering functon, previous rendering function in `base_ant.py` file\n",
    "4. Goal of vision is depth perception -> maybe no vision encoder, given depth perception directly -> map of length, create a vector from the starting point with a straight line.\n",
    "\n",
    "### CPU Side of Improvement:\n",
    "1. CPU Mujoco still uses rendering, C++ binding (multi-thread binding), even quite similar to MJX\n",
    "2. Goal is imitation learning, start with cpu based first because rendering can work with it and old data is presented.\n",
    "3. MJX have it’s own strength in doing, but cpu based Mujoco is good for imitation learning\n",
    "4. Simulation takes step in cpu, nn in gpu when inferencing, some transfer between data there\n",
    "\n",
    "### TorchRL Environment:\n",
    "1. Torch jitted step is 10 times slower compared to jitted Brax step -> may not be a issue when having full scale full parallel environment?\n",
    "2. Torch have many wrappers -> More wrappers means more code\n",
    "3. **Sticking with Brax is good for now**\n",
    "\n",
    "### What do we have MJX based Mujoco for?\n",
    "1. GPU MJX is just circle\n",
    "2. Software is still immature\n",
    "3. Hope  for GPU based is that it will be much easier to be used and faster to be used in the future once support is provided. Now it may be too early.\n",
    "4. **Use for biologically learning algorithm/ biologically inspired architecture.**\n",
    "\n",
    "**Goal: Wrap up MJX stuff, get the vision to work with gap stuff, get rendering to be faster, if not then depth perception -> get rat to jump over some gap.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Training Configuration & Rendor Trajectory\n",
    "***\n",
    "\n",
    "This is a direct easy view of some of the most important hyperparametyer tuning that we need todo. Also visit PPO Documentation for more details (https://github.com/google/brax/blob/main/brax/training/agents/ppo/train.py).\n",
    "1. `num_env`:\n",
    "    - Number of environment is refering to the number of parrallel environment that the agent is traine on. In another word, it is how many instances of the registered environment that have being activated and trained in the same time.\n",
    "    - It creates more robust and diverse policy\n",
    "    - Agent learns quicker because it gathers experiences from all of them and just choose the best policy\n",
    "2. `num_timesteps`:\n",
    "    - Total number of interactions that will happen between the agent and environment\n",
    "3. `eval_every`:\n",
    "    - eval_every is the learning time, it is how often do we update the policy parameter. This is the same with the horizon idea N in the pytorch simple version PPO we implemented\n",
    "4. `episode_length`:\n",
    "    - episode length is the number of timesteps that constitute one episode. In here it would be 1000 * 10_000 episodes.\n",
    "5. `num_evals`:\n",
    "    - In total how many evals there are, should be num_timesteps/eval_every\n",
    "6. `batch_size`:\n",
    "    - Batch size is the number of samples extracted from the \"replay buffer\" for calling as previous experiences everytime during bellman equation optimization for 1 batch.\n",
    "    - This 1 batch is used for 1 SGD only, we SGD multiple times with multiple subsamples or \"mini batches\"\n",
    "    - In brax implementation of ppo, it doesn't have an replay buffer, but the idea is the same.\n",
    "7. `num_minibatches`:\n",
    "    - This is how many batches there are by splitting all data in the replay buffer to little chunks\n",
    "    - This is also how many times the SGD is run\n",
    "    - num_batch * batch_size = all_data\n",
    "    - as replay buffer increase, num_minibatches is the same, each batch_size increases\n",
    "8. `unroll_length`:\n",
    "    - The number of timesteps to unroll in each environment. Instead of looking at one time step on the trajectory, unroll_length helps to look at n-steps on the trajectory and collect all the data at once for computational efficiency\n",
    "\n",
    "**Number of evaluations is proportion to the number of frames that can be rendered later on. However, it does not mean that eval at 500 steps and eval at 1000 steps's differences is just adding frames, it generate completely new insights and new learnings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are quite some big update on Brax_mjx\n",
    "class Walker(PipelineEnv):\n",
    "  '''\n",
    "  This is greatly coustomizable of what reward you want to give: reward engineering\n",
    "  '''\n",
    "  def __init__(\n",
    "      self,\n",
    "      forward_reward_weight=5.0,\n",
    "      ctrl_cost_weight=0.1,\n",
    "      healthy_reward=0.5,\n",
    "      terminate_when_unhealthy=True, # should be false in rendering\n",
    "      healthy_z_range=(0.0, 1.0), # healthy reward takes care of not falling, this is the contact_termination in dm_control\n",
    "      train_reward=5.0,\n",
    "      reset_noise_scale=1e-2,\n",
    "      exclude_current_positions_from_observation=True,\n",
    "      **kwargs,):\n",
    "    '''\n",
    "    Defining initilization of the agent\n",
    "    '''\n",
    "\n",
    "    mj_model = physics.model.ptr\n",
    "    # this is directly a mj_model already of type mujoco_py.MjModel (This is already a MJModel, same as previously in brax)\n",
    "    # the original xml load is directly creaing an new MjModel instance, which carries the configuration of everything, including mjtCone\n",
    "    # but this pass in one doesn't, it uses the default mjCONE_PYRAMIDAL, but MjModel now uses the eliptic model, so reset is needed\n",
    "\n",
    "    # solver is an optimization system\n",
    "    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_NEWTON #.mjSOL_CG\n",
    "    mj_model.opt.cone = mujoco.mjtCone.mjCONE_PYRAMIDAL # Read documentation\n",
    "\n",
    "    #Iterations for solver\n",
    "    mj_model.opt.iterations = 2\n",
    "    mj_model.opt.ls_iterations = 4\n",
    "\n",
    "    sys = mjcf_brax.load_model(mj_model)\n",
    "\n",
    "    # Defult framne to be 5, but can self define in kwargs\n",
    "    physics_steps_per_control_step = 3\n",
    "    \n",
    "    kwargs['n_frames'] = kwargs.get(\n",
    "        'n_frames', physics_steps_per_control_step)\n",
    "    kwargs['backend'] = 'mjx'\n",
    "\n",
    "    # Parents inheritence from MjxEnv class\n",
    "    #super().__init__(model=mj_model, **kwargs)\n",
    "    super().__init__(sys, **kwargs)\n",
    "\n",
    "    # Global vraiable for later calling them\n",
    "    self._model = mj_model\n",
    "    self._forward_reward_weight = forward_reward_weight\n",
    "    self._ctrl_cost_weight = ctrl_cost_weight\n",
    "    self._healthy_reward = healthy_reward\n",
    "    self._terminate_when_unhealthy = terminate_when_unhealthy\n",
    "    self._healthy_z_range = healthy_z_range\n",
    "    self._train_reward = train_reward\n",
    "    self._reset_noise_scale = reset_noise_scale\n",
    "    self._exclude_current_positions_from_observation = (exclude_current_positions_from_observation)\n",
    "\n",
    "  def reset(self, rng: jp.ndarray) -> State:\n",
    "    \"\"\"Resets the environment to an initial state.\"\"\"\n",
    "\n",
    "    #Creating randome keys\n",
    "    #rng = random number generator key for starting random initiation\n",
    "    rng, rng1, rng2 = jax.random.split(rng, 3)\n",
    "\n",
    "    low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
    "\n",
    "    #Vectors of generalized joint position in the configuration space\n",
    "    qpos = self.sys.qpos0 + jax.random.uniform(\n",
    "        rng1, (self.sys.nq,), minval=low, maxval=hi\n",
    "    )\n",
    "\n",
    "    #Vectors of generalized joint velocities in the configuration space\n",
    "    qvel = jax.random.uniform(\n",
    "        rng2, (self.sys.nv,), minval=low, maxval=hi\n",
    "    )\n",
    "\n",
    "    data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "    #Reset everything\n",
    "    obs = self._get_obs(data, jp.zeros(self.sys.nu)) #['proprioceptive']\n",
    "    reward, done, zero = jp.zeros(3)\n",
    "    metrics = {\n",
    "        'forward_reward': zero,\n",
    "        'reward_linvel': zero,\n",
    "        'reward_quadctrl': zero,\n",
    "        'reward_alive': zero,\n",
    "        'x_position': zero,\n",
    "        'y_position': zero,\n",
    "        'train_reward': zero,\n",
    "        'distance_from_origin': zero,\n",
    "        'x_velocity': zero,\n",
    "        'y_velocity': zero,\n",
    "    }\n",
    "    return State(data, obs, reward, done, metrics) # State is a big wrapper that contains all information about the environment\n",
    "\n",
    "  def step(self, state: State, action: jp.ndarray) -> State: # push towards another State\n",
    "    \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
    "    #Previous Pipeline\n",
    "    data0 = state.pipeline_state\n",
    "\n",
    "    #Current pipeline state, step 1\n",
    "    #Looking at the documenttaion of pipeline_step, \"->\" means return a modified State\n",
    "    data = self.pipeline_step(data0, action)\n",
    "\n",
    "    #Running forward (Velocity) tracking base on center of mass movement\n",
    "    com_before = data0.subtree_com[3]\n",
    "    com_after = data.subtree_com[3]\n",
    "\n",
    "    #print(data.data)\n",
    "    \n",
    "    velocity = (com_after - com_before) / self.dt\n",
    "    forward_reward = self._forward_reward_weight * velocity[0]\n",
    "\n",
    "    #Reaching the target location distance using eucledian distance and considering moving backwards\n",
    "    # def euclidean_distance(point1, point2):\n",
    "    #   squared_diff = jp.square(point1 - point2)\n",
    "    #   distance = jp.sqrt(jp.sum(squared_diff))\n",
    "    #   return distance\n",
    "    # distance_reward = self._distance_reward * euclidean_distance(com_before, com_after)\n",
    "    # def negate_distance_reward(_):\n",
    "    #   return -distance_reward\n",
    "    # def identity_distance_reward(_):\n",
    "    #   return distance_reward\n",
    "    # condition = jp.dot(com_before, com_after) < 0\n",
    "    # distance_reward = jax.lax.cond(condition, \n",
    "    #                           negate_distance_reward, \n",
    "    #                           identity_distance_reward, \n",
    "    #                           None)\n",
    "\n",
    "    train_reward = self._train_reward * self.dt # as more training, more rewards\n",
    "\n",
    "    #Height being healthy\n",
    "    min_z, max_z = self._healthy_z_range\n",
    "    is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
    "    is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
    "\n",
    "    #Termination condition\n",
    "    if self._terminate_when_unhealthy:\n",
    "      healthy_reward = self._healthy_reward\n",
    "    else:\n",
    "      healthy_reward = self._healthy_reward * is_healthy\n",
    "\n",
    "    #Control force cost\n",
    "    ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
    "\n",
    "    #Feedback from env\n",
    "    obs = self._get_obs(data, action)\n",
    "    reward = forward_reward + train_reward + healthy_reward - ctrl_cost\n",
    "\n",
    "    #print(obs)\n",
    "\n",
    "    #Termination State\n",
    "    done = 1.0 - is_healthy if self._terminate_when_unhealthy else 0.0\n",
    "\n",
    "    state.metrics.update(\n",
    "        forward_reward=forward_reward,\n",
    "        reward_linvel=forward_reward,\n",
    "        reward_quadctrl=-ctrl_cost,\n",
    "        reward_alive=healthy_reward,\n",
    "        x_position=com_after[0],\n",
    "        y_position=com_after[1],\n",
    "        train_reward=train_reward,\n",
    "        distance_from_origin=jp.linalg.norm(com_after),\n",
    "        x_velocity=velocity[0],\n",
    "        y_velocity=velocity[1],\n",
    "    )\n",
    "    return state.replace(pipeline_state=data, obs=obs, reward=reward, done=done)\n",
    "\n",
    "  def _get_obs(self, data: mjx.Data, action: jp.ndarray) -> jp.ndarray:\n",
    "    \"\"\"environment feedback of observing walker's proprioreceptive and vision data\"\"\"\n",
    "\n",
    "    # Vision Data Mujoco Version\n",
    "    # passed in data is a pipeline_state.data object, pipeline_state is the sate\n",
    "    renderer = mujoco.Renderer(model = self._model)\n",
    "\n",
    "    # this here is the correct format, need qpos in calling\n",
    "    #d = mjx.get_data(self._model, data)\n",
    "    d = mujoco.MjData(self._model)\n",
    "\n",
    "    mujoco.mj_forward(self._model, d)\n",
    "    renderer.update_scene(d, camera=3) # can call via name too!\n",
    "    image = renderer.render()\n",
    "    image_jax = jax.numpy.array(image)\n",
    "    print(f'image out of mujoco is {image_jax.shape}')\n",
    "    # cam = mujoco.MjvCamera()\n",
    "\n",
    "    # fake_image = jax.numpy.array(np.random.rand(64, 64, 3))\n",
    "    # image_jax = fake_image.flatten() # fit into jp array\n",
    "\n",
    "    o_height, o_width, _ = 240,320,3\n",
    "    c_x,  c_y = o_width//2, o_height//2\n",
    "    cropped_jax_image = image_jax[c_y-32:c_y+32, c_x-32:c_x+32, :]\n",
    "    print(f'image cropped {cropped_jax_image.shape}')\n",
    "\n",
    "    image_jax = cropped_jax_image.flatten()\n",
    "    image_jax_noise = image_jax * 1e-12 # noise added\n",
    "    print(f'image cropped flatened {image_jax_noise.shape}')\n",
    "\n",
    "    # Proprioreceptive Data\n",
    "    position = data.qpos\n",
    "    velocity = data.qvel\n",
    "    if self._exclude_current_positions_from_observation:\n",
    "      position = position[2:]\n",
    "\n",
    "    proprioception = jp.concatenate([position, velocity])\n",
    "    \n",
    "    # buffer_proprioception = jax.numpy.array(np.random.rand(27,))\n",
    "\n",
    "    # num = (230427-(27+16)) # image size - (proprioreception + activation parameter)\n",
    "    # buffer_vision = jax.numpy.array(np.random.rand(num,))\n",
    "\n",
    "    # # for shape call in train.py of ppo\n",
    "    # shape = jp.concatenate([proprioception,image_jax]).shape[0] # shape -1 is one number, give as shape tuple\n",
    "\n",
    "    # full = jp.concatenate([proprioception,image_jax])\n",
    "  \n",
    "    # return BraxData(\n",
    "    #   proprioception = proprioception,\n",
    "    #   vision = image_jax,\n",
    "    #   full=full,\n",
    "    #   buffer_proprioception = buffer_proprioception,\n",
    "    #   buffer_vision = buffer_vision,\n",
    "    #   shape = (128, shape) # this works, but there is a type check in jax\n",
    "    # )\n",
    "\n",
    "    return jp.concatenate([proprioception, image_jax_noise])\n",
    "\n",
    "# Registering the environment setup in env as humanoid_mjx\n",
    "envs.register_environment('walker', Walker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendoring a Rollout\n",
    "\n",
    "You need `ffmpeg` to render a rollout, mac rendering can avoid the GL error on Linux serevr,  now rendoring on mac works just fine!\n",
    "The environment actually works -> establishes and step successfuly. \n",
    "- For Linux Server rendering, you need to run this pipeline in a Gpu based and having neccessary colab import and installation with handling GL error.\n",
    "\n",
    "dm_control model did implement `camera`, but the name is different from brax's model, so we need to find the specific name for the specific model (camera is usually implemeneted in the walker's xml file, it is not something that is normally directly ccaried by MjModle, but it can be set as well):\n",
    "\n",
    "camera default argument is actually None in brax documentation here in this [line](https://github.com/google/brax/blob/a89322496dcb07ac5a7e002c2e1d287c8c64b7dd/brax/envs/base.py#L205)\n",
    "\n",
    "Rendering now works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envs.get_environment(env_name='walker')\n",
    "\n",
    "# define the jit reset/step functions\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "\n",
    "# initialize the state\n",
    "state = jit_reset(jax.random.PRNGKey(0))\n",
    "\n",
    "#Creating an container for rollout states\n",
    "rollout = [state.pipeline_state]\n",
    "\n",
    "# grab a trajectory\n",
    "for i in (range(500)):\n",
    "    ctrl = jax.numpy.array(np.random.uniform(-1,1, env.sys.nu))\n",
    "    # -1 * jp.ones(env.sys.nu) is just gravity\n",
    "    # positive is flattening\n",
    "    # negative grabs tight\n",
    "    state = jit_step(state, ctrl)\n",
    "    rollout.append(state.pipeline_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media.show_video(env.render(rollout,camera=1), fps=1.0 / env.dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Stage 5: Moving to TorchRL Rodent\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theory is what guide us through the millions attempt of practical side, guiding a fast and correct path:\n",
    "\n",
    "***Solving a ML problem itself is a no-hard/non-convex optimization problem. Whne no theory is presented, you are essetntially doing np.random.chosie to find a path, but when theory is presented, you have the ability to search and optimize, to do A\\* Search, and to do MCTS***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchRL\n",
    "\n",
    "1. Data Collection and Buffer\n",
    "    - The `SyncDataCollector` is responsible for collecting data from the environment according to the policy defined by the actor model. It manages the interactions with the environment and collects states, actions, rewards, and other relevant information.\n",
    "    - `TensorDictReplayBuffer` with `LazyMemmapStorage` is used to store the collected data efficiently. This setup facilitates the storage and retrieval of experience tuples for training the models.\n",
    "\n",
    "2. Loss Calculation and Optimization\n",
    "    - `GAE` (Generalized Advantage Estimation) is used for calculating the advantage estimates, which represent the relative value of taking specific actions in given states. GAE is a technique that reduces the variance of the advantage estimates while allowing for efficient computation.\n",
    "    - The `ClipPPOLoss` module implements the clipped surrogate objective of PPO, which limits the updates to the policy by clipping the probability ratio, thus preventing disruptive updates.\n",
    "    - Separate `Adam` optimizers for the actor and critic models are created, emphasizing the decoupled nature of policy and value function updates in Actor-Critic methods.\n",
    "\n",
    "3. Training Loop\n",
    "    - **Data Collection**: Collecting data from the environment using the policy.\n",
    "    - **Advantage Computation**: Computing advantages using GAE.\n",
    "    - **Buffer Update**: Storing experiences in the replay buffer.\n",
    "    - **Policy and Value Function Updates**: Sampling mini-batches from the buffer and performing gradient updates on both the actor and critic using the PPO loss. The script includes mechanisms to adjust the learning rate and clipping parameter dynamically.\n",
    "\n",
    "4. Logging and Evaluation\n",
    "    - Logging: The script uses a logging setup that can be integrated with different backends, including WandB, for tracking experiments. It logs various metrics such as rewards, episode lengths, loss components, and learning rates.\n",
    "    - Evaluation: The policy is periodically evaluated on a test environment, and the evaluation rewards are logged. This evaluation phase is crucial for monitoring the policy's performance in an unbiased setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Algorithm Revisit\n",
    "<center><img src=images/ppo.png width=80%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why TorchRL:\n",
    "1. TorchRL is highly **modularized**, all the hyperparameters are all collected back to the config yaml file\n",
    "    - Quite easy training loop comparing to brax\n",
    "2. Brax runs **30 times faster** than TorchRL, but checkpoint system is bad\n",
    "3. Brax **checkpoint system** loading very bad (weights saved, but weights not loaded, previous information in checkpoints not been used later in training), generalization doesn't work, so we need to adapt TorchRL training.\n",
    "    - Brax runs during PPO algorithm training, but in new condition, we cannot replicate the result, problematic\n",
    "    - TorchRL checkpoint system works\n",
    "4. TorchRl provides **network flexiability**\n",
    "    - We can implement multiple layer higher attention and lower network directly usability of motor control\n",
    "    - Brax writes jax using **functional programming** (calling nested functions for speed purposes), our understanding of it is not enough (different logic), but TorchRL uses **object-oriented programing**, which is a concept that we are much more familier and easy to use with.\n",
    "5. **Multiple layer learning**\n",
    "    - One NN learns low level (walking, balance), one is action of playing on keyboard\n",
    "    - One NN learns higher level (goal driven -> base on task how to do), one is knowing how to play the piano\n",
    "    - They share embedding space to learn together\n",
    "6. Very good documentation and examples of how things are done\n",
    "    - General documentaion: https://pytorch.org/rl/\n",
    "    - Mujoco specific documentation: https://github.com/pytorch/rl/tree/main/sota-implementations/ppo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Stage 5: TorchRL Training Loop\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop\n",
    "Demistifying TorchRL, all components is customizable and adjustable\n",
    "\n",
    "The main training loop for TorchRL is very very simple and direct, cna be directly transformed from the PPO paper\n",
    "- Step MDP\n",
    "- Collect radnom sample data using **probabilistic policy player** with DataCollector -> ReplayBuffer\n",
    "    - Compute rewards via **TD bellman equation**\n",
    "    - Compute advantage (GAE) via comparison using ReplayBuffer data\n",
    "        - Loop over the collected to compute loss values\n",
    "        - Back propagate (SGD on value using MSE (Adam)) -> better GAE understanding what is good\n",
    "        - Optimize policy by maximize ppo_clip objective (uses GAE) -> going to the direction that GAE points at (Adam)\n",
    "            - Forward pass: calculate loss\n",
    "            - Backward pass: SGD update network and zero out gradient\n",
    "            - To preserve creativity with alignment: PPO Loss = Value Loss + Policy Loss + Entropy Loss\n",
    "        - Repeat\n",
    "    - Repeat\n",
    "- Repeat\n",
    "\n",
    "Tutorial link: https://pytorch.org/rl/tutorials/coding_ppo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO_Models\n",
    "In a nn.sequential fashion, not parallel\n",
    "<center><img src=images/ppo_models_new.png width=60%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal Policy Optimization Loss Functions\n",
    "PPO Algoriithm: https://spinningup.openai.com/en/latest/algorithms/ppo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Algorithm Representation:\n",
    "<center><img src=images/ppo_2.png width=60%></center>\n",
    "\n",
    "The key equation to update PPO is $\\theta_{k+1} = \\arg \\max_{\\theta} \\underset{s,a \\sim \\pi_{\\theta_k}}{{\\mathrm E}}\\left[L(s,a,\\theta_k, \\theta)\\right]$, which tells us that we are taking the max of such objective, maximizing rewards, but there are a few things we need to do involving:\n",
    "- Taking the expectation of the sum of the minimum between an unclip **imporatnace sampling ratio** times GAE and an clipped importance sampling ratio using ($1-\\epsilon$) and ($1+\\epsilon$) times GAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full PPO Algorithm Representation:\n",
    "This is. the full representation of the PPO Algorithm\n",
    "<center><img src=images/ppo_3.svg width=60%></center>\n",
    "\n",
    "#### Policy Optimization\n",
    "This representation is an **off-policy** version of ppo in some sense, uisng **important sampling ratio**, **clipping**, and **KL Divergence** to make a more preservative approach on making updates:\n",
    "\n",
    "- The **important sampling ratio** $\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}$ compares the action taken by the current policy and the action taken by the previous policy, which indicates how much more or less likely the new policy is to take action $a$ under state $s_t$ compared to the previosu policy, which **bounds the action from been too drastic change**.\n",
    "\n",
    "- $g(\\epsilon, A^{\\pi_{\\theta_k}}(s,a))$ is a clipped version of this ratio, which uses the advantage function $A^{\\pi_{\\theta_k}}(s,a)$ and a clipping parameter $\\epsilon$ to **prevent the new policy from moving too far away from the old policy**.\n",
    "\n",
    "- Again, we are taking the $min$ of it, showing a more **preservative** approach.\n",
    "\n",
    "- The $D_k$ value signifies **KL Divergence** that measures ho big of a change did the update cause on the policy (statistical method) -> bigger change have more punishments.\n",
    "\n",
    "- Reacall that **policy gradient** is about the **expectation of the reward**. The Expectation here is formed by doing empirical sample mean over a finite batch of trajectories collected under the previous policy from the replay buffer.\n",
    "\n",
    "#### Value Gradient Decent\n",
    "On the other hand, the value network is updated via **Mean Square Error**:\n",
    "\n",
    "- Comparing randomly **new** sampled reward with **different path computed by current policy** at the **current state** (that try to get reward to the best by default bellman update idea) with the current known reward at current state:\n",
    "\n",
    "    1. ***One interpretation***: from looking at the setting , here is where the assumption of bellman equation comes in, bellman update is always assuming getting the better next state, so compare currently have reward with randomly sampled theoritically best current (new sample with $max(Q(s,a))$) makes sense. **Bellman update ganrantees helping the agent getting more reward when updating like this -> help to expand understanding of the MDP**\n",
    "\n",
    "    2. ***Second interpretation***: from a more search perspective, this is **expanding the understanding of the MDP**, keep trying to reshape the value neural network towards the direction that have higher reward -> some what like a **learning or adapted heuristic** idea.\n",
    "\n",
    "- The $D_k$ value signifies **KL Divergence** that measures ho big of a change did the update cause on the value (statistical method) -> bigger change have more punishments.\n",
    "\n",
    "- We try to minimize this value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Wrapper Setup: Fundamentals\n",
    "Environment References: https://pytorch.org/rl/reference/envs.html\n",
    "\n",
    "TensorDictModule References: https://pytorch.org/tensordict/reference/generated/tensordict.nn.TensorDictModule.html\n",
    "\n",
    "1. Using the rodent customed environment from brax in `Rodent_Env_Brax.py`\n",
    "2. Transform using `TransformedEnv`, the transformed environment will inherit the device and meta-data of the wrapped environment, and transform these depending on the sequence of transforms it contains.\n",
    "3. **usage of `TensorSpec` to keep track of all data.**\n",
    "    - Think abut what data format is it giving, vector? scalar?\n",
    "    - Try it out in `torch_explore.ipynb` of the separate interaction with the observation space and the environment.\n",
    "    - Replicate the **error** first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=images/env_update.png width=60%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop: Policy\n",
    "\n",
    "PPO utilizes a **stochastic policy** to handle exploration. This means that our neural network will have to output the parameters of a distribution, rather than a single value corresponding to the action taken. As the data is continuous, we use a **Tanh-Normal distribution** (same with brax) to respect the action space boundaries. TorchRL provides such distribution, and the only thing we need to care about is to build a neural network that outputs the right number of parameters for the policy to work with (a location, or mean, and a scale):\n",
    "\n",
    "```python\n",
    "actor_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(2 * env.action_spec.shape[-1], device=device),\n",
    "    NormalParamExtractor(),\n",
    ")\n",
    "```\n",
    "The policy can use a **data carrier** to talk to the environment\n",
    "\n",
    "```python\n",
    "policy_module = TensorDictModule(\n",
    "    actor_net, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"]\n",
    ")\n",
    "```\n",
    "\n",
    "Then again we create a **ProbabilisticPolicy** module as a bigger wrapper:\n",
    "\n",
    "```python\n",
    "policy_module = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"min\": env.action_spec.space.low,\n",
    "        \"max\": env.action_spec.space.high,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    "    # we'll need the log-prob for the numerator of the importance weights\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop: Value\n",
    "The value network is a crucial component of the PPO algorithm, even though it won’t be used at inference time. This **module will read the observations and return an estimation of the discounted return for the following trajectory**. This allows us to amortize learning by relying on the some utility estimation that is **learned on-the-fly during training**.\n",
    "\n",
    "Our value network share the same structure as the policy, but for simplicity we assign it its own set of parameters.\n",
    "\n",
    "```python\n",
    "value_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(1, device=device),\n",
    ")\n",
    "\n",
    "value_module = ValueOperator(\n",
    "    module=value_net,\n",
    "    in_keys=[\"observation\"],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop: Data Collector\n",
    "TorchRL provides a set of DataCollector classes and these classes execute three operations:\n",
    "1. Reset an environment\n",
    "2. Compute an action given the latest observation\n",
    "3. Execute a step in the environment\n",
    "\n",
    "They repeat the last two steps until the environment signals a stop (or reaches a done state). They allow you to control how many frames to collect at each iteration (through the `frames_per_batch` parameter), when to reset the environment (through the `max_frames_per_traj` argument), on which device the policy should be executed, etc. They are also designed to work efficiently with batched and multiprocessed environments.\n",
    "\n",
    "- The simplest data collector is the `SyncDataCollector`: it is an iterator that you can use to get batches of data of a given length, and that will stop once a total number of frames (total_frames) have been collected.\n",
    "- Other data collectors (MultiSyncDataCollector and MultiaSyncDataCollector) will execute the same operations in synchronous and asynchronous manner over a set of multiprocessed workers.\n",
    "\n",
    "As for the policy and environment before, the data collector will return TensorDict instances with a total number of elements that will match frames_per_batch. Using TensorDict to pass data to the training loop allows you to write data loading pipelines that are 100% oblivious to the actual specificities of the rollout content.\n",
    "\n",
    "```python\n",
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy_module,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    split_trajs=False,\n",
    "    device=device,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop: Replay Buffer\n",
    "Replay buffers are a common building piece of off-policy RL sota-implementations. In on-policy contexts, a replay buffer is refilled every time a batch of data is collected, and its data is repeatedly consumed for a certain number of epochs.\n",
    "\n",
    "TorchRL’s replay buffers are built using a common container `ReplayBuffer` which takes as argument the components of the buffer:\n",
    "1. a storage\n",
    "2. a writer\n",
    "3. a sampler\n",
    "4. possibly some transforms\n",
    "\n",
    "Only the storage (which indicates the replay buffer capacity) is mandatory. We also specify a sampler without repetition to avoid sampling multiple times the same item in one epoch. Using a replay buffer for PPO is not mandatory and we could simply sample the sub-batches from the collected batch, but using these classes make it easy for us to build the inner training loop in a reproducible way.\n",
    "\n",
    "```python\n",
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(max_size=frames_per_batch),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop: Loss Functions\n",
    "The PPO loss can be directly imported from TorchRL for convenience using the `ClipPPOLoss` class. This is the easiest way of utilizing PPO: it hides away the mathematical operations of PPO and the control flow that goes with it.\n",
    "\n",
    "PPO requires some “advantage estimation” to be computed. In short, an advantage is a value that reflects an expectancy over the return value while dealing with the bias / variance tradeoff.\n",
    "- (1) build the advantage module, which utilizes our value operator\n",
    "- (2) pass each batch of data through it before each epoch.\n",
    "\n",
    "The GAE module will update the input tensordict with new \"advantage\" and \"value_target\" entries. The \"value_target\" is a gradient-free tensor that represents the empirical value that the value network should represent with the input observation.\n",
    "- Both of these will be used by ClipPPOLoss to return the policy and value losses.\n",
    "\n",
    "```python\n",
    "advantage_module = GAE(\n",
    "    gamma=gamma, lmbda=lmbda, value_network=value_module, average_gae=True\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy_module,\n",
    "    critic_network=value_module,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim, total_frames // frames_per_batch, 0.0\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Stage 5: CPU Based Mujoco With Torch RL\n",
    "***\n",
    "\n",
    "***Always remanber one thing, computers, even to the bottom system level is not magic, it works on logic, if there is an error, comparison between logics and chase bit by bit is what gets the error out. All \"bugs\" can be cebugged using logic***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Modified Mujoco From Source\n",
    "The change made is from python packages -> C++ -> access of data in Mujoco C++ (no changes to the data simulation process in Mujoco (cmake no change))\n",
    "\n",
    "build (built from Mujoco) -> release (for compilation & distribution) -> python packages (make compiled C++ Mujoco into python packages) -> TorchRL customized Env\n",
    "\n",
    "1. Building Mujoco from source (https://mujoco.readthedocs.io/en/stable/programming/index.html)\n",
    "    1. Install cmake by `sudo apt install cmake`\n",
    "    2. Clone Emil’s modified Mujoco repository from GitHub with branch of “feature/batched-render” (https://github.com/emiwar/mujoco/tree/feature/batched-render )\n",
    "    3. Create a new build & release directory and cd into it\n",
    "    4. Run `cmake ../mujoco` to get all dependency and ready to build\n",
    "        - There are 3 places in C++ where errors may occur: when building, when compiling, and when running, the previous 2 usually deal with dependencies missing.\n",
    "        - Run `apt-cache search <packages>` to find the version\n",
    "        - Run `sudo apt install <package>` to install <packages>\n",
    "    5. Run `cmake --build .` to actually build the package\n",
    "    6. Export the build version: `cmake ../mujoco -DCMAKE_INSTALL_PREFIX=../release`\n",
    "    7. After building, install with `cmake --install`\n",
    "    8. Run `cd release`, you should see normal Mujoco C++ version now\n",
    "\n",
    "2. Building python packages from compiled C++ code (When need to reflect changes in the system, only do this step is necessary)\n",
    "    1. In the root file, create a new venv with `python -m venv cpu_mujoco`\n",
    "    2. Run `source cpu_mujoco/bin/activate` to activate the virtual environment\n",
    "    3. Run `cd mujoco/python`\n",
    "    4. Run `bash make_sdist.sh` to make the python package\n",
    "    5. Run `cd dict` and the python package `mujoco-3.1.4.tar.gz` should be there\n",
    "        - Set env flag with `export MUJOCO_PLUGIN_PATH=~/release`\n",
    "        - Set env flag with `export MUJOCO_PATH=~/release`\n",
    "    6. Run `pip install mujoco-3.1.4.tar.gz` to install the python package that can be used just like the normal Mujoco python binding version.\n",
    "\n",
    "3. Now you can create TorchRL customized environment because import mujoco, mujoco.simulationpool would all work\n",
    "This branch https://github.com/emiwar/torchrl-mujoco-benchmark/tree/validateEnv should create the customized environment written by Emil\n",
    "    - Pushing towards imitation learning and PPO on CPU based Mujoco can work now\n",
    "    - May need to set mujoco on mac as well to have easy changable system there\n",
    "\n",
    "## Multi-thread\n",
    "- **Cores** are physical processing units.\n",
    "- **Threads** are virtual sequences of instructions given to a CPU. \n",
    "- **Multithreading** allows for better utilization of available system resources by dividing tasks into separate threads and running them in parallel. ***We have 6 multishread*** running for cpu mujoco now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Stage 5: Report for TorchRL\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report from Week 2 (TorchRL算法)\n",
    "1. TorchRL uses normal Mujoco, which is not a vectorized implementation\n",
    "    - Vectorized code in source code of TorchRL\n",
    "    - Brax is from dm_control and TorchRL is from Meta, there are some delayed in the updating of the mdoels, so that need  to be fixed\n",
    "    - Implemented (in `base.py` and `jax_util.py`)\n",
    "2. Brax does train, rendering rollout, load echeckpoint does not reflect ability in evaluator (PPO is also working)\n",
    "3. Rodent runing first\n",
    "    - Check NN\n",
    "    - Check Bindings and training loop\n",
    "\n",
    "4. Main training loop setup with brax/torchrl/hydra\n",
    "    - torchrl not updated for brax latest version, need to change some default stuff in packages, maybe a forck directly would be better?\n",
    "    - directly setting it in serevr is quite complicated\n",
    "    - use vnl_adapted_torch_rl @ https://github.com/KevinBian107/torch_rl_vnl\n",
    "    - problme not the same with before, after adjustment, server can run perfectly on notebook, the problem is the parralel environment creating\n",
    "    - seems like. non-parralel environment encounter the same issue when using just mujoco_util implementation.\n",
    "    - **figured out the issue** by comparing the code in notebook and. in the python file, the environment Rodent is not regestered, which is a unique thing to brax, after regester and fix folder, pass this issue.\n",
    "\n",
    "5. Segementation linux system issue?\n",
    "\n",
    "6. main issue currently in `make_ppo_models` function, which refers to the basic `make_ppo_modules_pixels` function\n",
    "    - Network structure need to be adjusted to be adapted\n",
    "    - Some dimensionality of network issues\n",
    "    - prior to 1260 all image data, 12288 image data\n",
    "    - `make_ppo_modules_pixels` function is now good\n",
    "\n",
    "7. There seems to be aproblem in th e`tanh` function that is common in the proof_environment and data collector\n",
    "\n",
    "    ```python\n",
    "    TypeError: distribution keywords and tensordict keys indicated by ProbabilisticTensorDictModule.dist_keys must match.Got this error message: \n",
    "        TanhNormal.__init__() got an unexpected keyword argument 'logits'\n",
    "    ```\n",
    "\n",
    "8. From this article: https://pytorch.org/rl/tutorials/coding_ppo.html\n",
    "    - We know that the distribution out put need an meu (loc) and an sigma (scale), however, the previous nn sequential module don't use `NormalParamExtractor()`, so no parameter is extracted, now it is extracted.\n",
    "\n",
    "9. Some shape mismatch issue:\n",
    "    ```python\n",
    "    mat1 and mat2 shapes cannot be multiplied (100x256 and 512x30)\n",
    "    ```\n",
    "    - The `NormalParamExtractor()` should be only at the policy net but not the common_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report from Week 3 (CPU训练，环境正确，算法修正)\n",
    "1. Emil's implementation using mujoco util and customized cpu mujoco works directly, but seems to have `NaN` issue.\n",
    "2. Environment setup with cpu mujoco works, directly on cluster, no other venv needed, use instructions on top\n",
    "3. No mujoco cpu environment stup local wise, only on cluster\n",
    "4. issues with `collector`?\n",
    "    - action szie should be 30, control size should also be 30\n",
    "    - use emil's rodent model with floor right now\n",
    "    - the control space seems to be always twice the size as the action space?\n",
    "5. The cpu baed mujoco seems to not be able to do parralel environment?\n",
    "    - the `torch_run_cpu.py` is very staright forward, training directly, no utilization of th  `utils` file\n",
    "    - **need help with trying to adapt to the torchrl training loop we have**\n",
    "\n",
    "6. **In principle cpu mujoco it could be used with parallel environments, but it would likely lose performance rather than gain any since our modified mujoco C++ code is multithreaded anyways.**\n",
    "\n",
    "7. Successful running with our implementation of the PPO training loop now! (**ask around and then maybe you get a new perspective!**)\n",
    "    - It was a error with `num_outputs = 2 * proof_environment.action_spec.shape[-1]`, Emil have the output of the policy module doubled, so the MLP that we use should also have that double\n",
    "    - Module issue, not system issue\n",
    "    - default thread is 6\n",
    "\n",
    "8. Now dealing with `NaN` issue\n",
    "    - `observation` is not `NaN`\n",
    "    - second step, all `loss` is `NaN`, first time `loss` is real values\n",
    "    ```python\n",
    "    tensor(-0.0053, grad_fn=<AddBackward0>)\n",
    "    tensor(inf, grad_fn=<AddBackward0>)\n",
    "    tensor(nan, grad_fn=<AddBackward0>)\n",
    "    ```\n",
    "    - `NaN` occurs because taking gradient shrinks it too much, going to inf value, thus `NaN`\n",
    "\n",
    "9. Directly use Emil's implementation with `ppo_mujoco.py` would work directly, calles `utils_mujoco.py` file\n",
    "    - No common module uses\n",
    "    - **modified based on our training loop**\n",
    "    - ***go through bit by bit, not complicated***\n",
    "\n",
    "10. Need to grab moreobservations?\n",
    "\n",
    "11. Adjust the server setup to allow check where center of mass is?\n",
    "    - Setup on cluster working now!\n",
    "    - Debugging is easier now!\n",
    "\n",
    "12. Camera set for dm_control, not shaking anymore!\n",
    "\n",
    "13. No matter in `mjx + brax` or in `cpu mujoco + torch_rl`, seems like dm_control processed training agent all have really strong ctrl and reaction jumps for some reason\n",
    "\n",
    "***Get more familier with the pytorch ppo training loop, system of cpu mujoco with intergration from dm_control, and what is needed and what is not in the training algorithm.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Stage 6: Imitation Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scott-yj-yang.github.io/DSC180-A08/repositories/\n",
    "\n",
    "## Two checkpoints are both working -> going to imitation learning\n",
    "1. **TorchRL with CPU Mujoco** - cpu mujoco setup!\n",
    "2. **Brax/Jax with MJX** - checkpoint normalized fixed!\n",
    "3. Attach dm_control\n",
    "\n",
    "## Imitation Learning\n",
    "1. 做不了BC，我们不知道action (only motion sequence, no action, use algorithm to imitate, trajectory close up)\n",
    "\n",
    "2. **IRL -> feature mapping, reward as how similar to the clip (stac motion sequence)**\n",
    "    - Time sequence of potion (qpos,  all joint position, just motion of movement as expert demonstration)\n",
    "    - Reward function做成representation learning -> not online, 不知道真实的reward是什么，只是尝试mimic expert (mujoco powered, but not relevant to the environment).\n",
    "        - Ultimately need todo switching，一会儿online，一会儿offline, maximize reward online and also biologically realistic way\n",
    "        - Then high level/low level intentional encoder decoder map\n",
    "\n",
    "2. Use COMIC paper -> multi clip? 随机抽2000 frame去学习\n",
    "\n",
    "3. Offline learning don’t know what’s good\n",
    "    - IRL is to find reward function that explains the expert well -> then do RL/PPO (do online learning)\n",
    "    - We are not try to guess the reward function, but rather constrain behavior using motion captured data and learn by online learning\n",
    "\n",
    "4. **We make it back to a supervised learning problem** -> compare expert pose track and with current learned position (用什么optimization都可以)\n",
    "    - Constrain low level policy now - Imitation learning\n",
    "    - High level for goal driven - Abstract learning (goal driven learning (WASD in gaming))\n",
    "    - Stochastic Clip Training (Multi-clip motion tracking):\n",
    "        - Randomly select starting frame and end frame\n",
    "        - Learn the whole move trajectory\n",
    "        - dm_control  support!\n",
    "\n",
    "***We are doing Offline Stochastic PPO***: Obviously the reward engineering shouldn't be something that is hardcoded, it should be learned again, just like in AlphaGo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imitation Learning With Brax\n",
    "- Repository from Charles: https://github.com/charles-zhng/Brax-Imitation\n",
    "- Repository from Talmo's Lab: https://github.com/talmolab/VNL-Brax-Imitation\n",
    "- MOCAP Data Set: https://drive.google.com/file/d/10WbPKUr9_1vH0c5KwuqpvdIlcRGEeE2k/view?usp=drive_link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
