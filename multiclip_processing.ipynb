{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-01 21:03:00.221876: W external/xla/xla/service/gpu/nvptx_compiler.cc:765] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.5.82). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "/root/anaconda3/envs/vnl/lib/python3.11/site-packages/jax/_src/interpreters/xla.py:155: RuntimeWarning: overflow encountered in cast\n",
      "  return np.asarray(x, dtypes.canonicalize_dtype(x.dtype))\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import jax\n",
    "from jax import jit, vmap\n",
    "\n",
    "import mujoco \n",
    "\n",
    "from mujoco import mjx\n",
    "\n",
    "from dm_control import mjcf\n",
    "from dm_control.locomotion.walkers import rescale\n",
    "\n",
    "import pickle\n",
    "\n",
    "from preprocessing.mjx_preprocess import process_clip\n",
    "\n",
    "# setup environment and stac data for preprocessing\n",
    "scale_factor = 0.9\n",
    "stac_path = \"./clips/all_snips.p\"\n",
    "\n",
    "with open(stac_path, \"rb\") as file:\n",
    "        d = pickle.load(file)        \n",
    "        data_qpos = d[\"qpos\"]\n",
    "        \n",
    "# Load rodent mjcf and rescale, then get the mj_model from that.\n",
    "# TODO: make this all work in mjx? james cotton did rescaling with mjx model:\n",
    "# https://github.com/peabody124/BodyModels/blob/f6ef1be5c5d4b7e51028adfc51125e510c13bcc2/body_models/biomechanics_mjx/forward_kinematics.py#L92\n",
    "root = mjcf.from_path(\"./assets/rodent.xml\")\n",
    "rescale.rescale_subtree(\n",
    "    root,\n",
    "    scale_factor,\n",
    "    scale_factor,\n",
    ")\n",
    "mj_model = mjcf.Physics.from_mjcf_model(root).model.ptr\n",
    "mj_data = mujoco.MjData(mj_model)\n",
    "\n",
    "    # Place into GPU\n",
    "mjx_model = mjx.put_model(mj_model)\n",
    "mjx_data = mjx.put_data(mj_model, mj_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_length = 250\n",
    "# Split clip like this if you want to run just once\n",
    "start_step = 0\n",
    "first_clip_qpos = data_qpos[start_step : start_step + clip_length]\n",
    "\n",
    "# jit the process_clip function\n",
    "jit_process_clip = jax.jit(process_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842, 250, 74)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape qposes to have the batch dimension and vmap the jitted function\n",
    "all_clips_qpos = data_qpos.reshape((-1, clip_length, mjx_model.nq))\n",
    "vmap_jit_process_clip = vmap(jit_process_clip, in_axes=(0, None, None))\n",
    "all_clips_qpos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clips = vmap_jit_process_clip(all_clips_qpos, mjx_model, mjx_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842, 250, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_clips.position.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving and loading (wip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def save_reference_clip_to_h5(filename, reference_clip):\n",
    "    \"\"\"\n",
    "    Save the contents of a ReferenceClip object to an .h5 file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the .h5 file to save to.\n",
    "        reference_clip (ReferenceClip): The ReferenceClip object to save.\n",
    "    \"\"\"\n",
    "    with h5py.File(filename, 'w') as hf:\n",
    "        for attr, value in reference_clip.__dict__.items():\n",
    "            if value is not None:\n",
    "                # Create a group for each batch\n",
    "                for batch_idx in range(value.shape[0]):\n",
    "                    #TODO: instead of batch_x, save as the name given by d[\"snips_order\"]\n",
    "                    # and save the order as its own thing at the top level\n",
    "                    group_name = f\"{attr}/batch_{batch_idx}\"\n",
    "                    hf.create_dataset(group_name, data=value[batch_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"clips/test_all_clips.h5\"\n",
    "save_reference_clip_to_h5(filename, all_clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.mjx_preprocess import ReferenceClip\n",
    "from jax import numpy as jp\n",
    "def load_reference_clip_from_h5(filename):\n",
    "    \"\"\"\n",
    "    Load the contents of an .h5 file into a ReferenceClip object.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the .h5 file to load from.\n",
    "\n",
    "    Returns:\n",
    "        ReferenceClip: The reconstructed ReferenceClip object.\n",
    "    \"\"\"\n",
    "    with h5py.File(filename, 'r') as hf:\n",
    "        clip = ReferenceClip()\n",
    "        for attr in clip.__dict__.keys():\n",
    "            batch_data = []\n",
    "            batch_idx = 0\n",
    "            while f\"{attr}/batch_{batch_idx}\" in hf:\n",
    "                batch_data.append(hf[f\"{attr}/batch_{batch_idx}\"][:])\n",
    "                batch_idx += 1\n",
    "            if batch_data:\n",
    "                setattr(clip, attr, jp.stack(batch_data))\n",
    "        return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position (842, 250, 3)\n",
      "quaternion (842, 250, 4)\n",
      "joints (842, 250, 67)\n",
      "body_positions (842, 250, 66, 3)\n",
      "velocity (842, 250, 3)\n",
      "joints_velocity (842, 250, 67)\n",
      "angular_velocity (842, 250, 3)\n",
      "body_quaternions (842, 250, 66, 4)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import fields\n",
    "\n",
    "for field in fields(all_clips):\n",
    "    print(field.name, getattr(all_clips, field.name).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 250, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    if len(x.shape) != 1:\n",
    "        return jax.lax.dynamic_slice_in_dim(\n",
    "                x,\n",
    "                0,\n",
    "                1,\n",
    "                )\n",
    "    return jp.array([])\n",
    "\n",
    "ref_traj = jax.tree_util.tree_map(f, all_clips)\n",
    "ref_traj.position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FrozenInstanceError",
     "evalue": "cannot assign to field 'position'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFrozenInstanceError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loaded_all_clips \u001b[38;5;241m=\u001b[39m \u001b[43mload_reference_clip_from_h5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m, in \u001b[0;36mload_reference_clip_from_h5\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     20\u001b[0m         batch_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_data:\n\u001b[0;32m---> 22\u001b[0m         \u001b[38;5;28;43msetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clip\n",
      "File \u001b[0;32m<string>:4\u001b[0m, in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n",
      "\u001b[0;31mFrozenInstanceError\u001b[0m: cannot assign to field 'position'"
     ]
    }
   ],
   "source": [
    "loaded_all_clips = load_reference_clip_from_h5(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
